{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 전처리 및 평가\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support as sk,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# 모델 관련\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# PyTorch 관련\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Keras/TensorFlow 관련\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "# TabNet\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "# AutoML\n",
    "import optuna\n",
    "\n",
    "# 기타\n",
    "import pickle\n",
    "\n",
    "# 주피터 환경 설정 (주피터에서만 유효)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "path=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Synthetic_model=['smote','adasyn','copulagan','ctgan','nbsynthetic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in Synthetic_model:\n",
    "   # 학습 데이터 불러오기\n",
    "    X_train = np.loadtxt(path+'\\\\Synthetic_data\\\\train_valid_test\\\\세명초_Nbsynthetic_X_train.txt', delimiter=',')\n",
    "    y_train = np.loadtxt(path+'\\\\Synthetic_data\\\\train_valid_test\\\\세명초_Nbsynthetic_y_train.txt', delimiter=',')\n",
    "\n",
    "    # 검증 데이터 불러오기\n",
    "    X_valid = np.loadtxt(path+'\\\\Synthetic_data\\\\train_valid_test\\\\세명초_Nbsynthetic_X_valid.txt', delimiter=',')\n",
    "    y_valid = np.loadtxt(path+'\\\\Synthetic_data\\\\train_valid_test\\\\세명초_Nbsynthetic_y_valid.txt', delimiter=',')\n",
    "\n",
    "    # 테스트 데이터 불러오기\n",
    "    X_test = np.loadtxt(path+'\\\\Synthetic_data\\\\train_valid_test\\\\세명초_Nbsynthetic_X_test.txt', delimiter=',')\n",
    "    y_test = np.loadtxt(path+'\\\\Synthetic_data\\\\train_valid_test\\\\세명초_Nbsynthetic_y_test.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 빈 리스트 생성하여 결과 저장\n",
    "model_results_list = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    \n",
    "    # 학습 데이터 불러오기\n",
    "\n",
    "    X_train = eval(generation+'_X_train')\n",
    "    y_train = eval(generation+'_y_train')\n",
    "\n",
    "    # 검증 데이터 불러오기\n",
    "    X_valid = eval(generation+'_X_valid')\n",
    "    y_valid = eval(generation+'_y_valid')\n",
    "\n",
    "    # 테스트 데이터 불러오기\n",
    "    X_test = eval(generation+'_X_test')\n",
    "    y_test = eval(generation+'_y_test')\n",
    "        \n",
    "    y_train=y_train-1\n",
    "    y_valid=y_valid-1\n",
    "    y_test=y_test-1\n",
    "    \n",
    "    \n",
    "    device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    tabnet_params = {\n",
    "    \"n_d\": 8,\n",
    "    \"n_a\": 8,\n",
    "    \"n_steps\": 5,\n",
    "    \"gamma\": 1.3,\n",
    "    \"cat_idxs\": [],\n",
    "    \"cat_dims\": [],\n",
    "    \"cat_emb_dim\": [],\n",
    "    \"n_independent\": 2,\n",
    "    \"n_shared\": 2,\n",
    "    \"epsilon\": 1e-15,\n",
    "    \"momentum\": 0.02,\n",
    "    \"lambda_sparse\": 0.001,\n",
    "    \"seed\": 0,\n",
    "    \"clip_value\": 1,\n",
    "    \"verbose\": 1,\n",
    "    \"optimizer_fn\": torch.optim.Adam,\n",
    "    \"optimizer_params\": {'lr': 0.02},\n",
    "    \"scheduler_fn\": None,\n",
    "    \"scheduler_params\": {},\n",
    "    \"mask_type\": 'sparsemax',\n",
    "    \"input_dim\": 6,\n",
    "    \"output_dim\": [3],\n",
    "    \"device_name\": 'auto',\n",
    "    \"n_shared_decoder\": 1,\n",
    "    \"n_indep_decoder\": 1,\n",
    "    \"grouped_features\": []\n",
    "    }\n",
    "    tabnet_clf = TabNetClassifier(**tabnet_params)\n",
    "    max_epochs=300\n",
    "    tabnet_clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    max_epochs=max_epochs ,\n",
    "    patience=200, # please be patient ^^\n",
    "    batch_size=5000,\n",
    "    virtual_batch_size=5000,\n",
    "    num_workers=1,\n",
    "    drop_last=False,\n",
    ")\n",
    "        \n",
    "    prediction=tabnet_clf.predict(X_test)\n",
    "    test_targets_np=y_test\n",
    "    with open(f'C:\\\\jupyter\\\\Elementary-student-Weight\\\\GAN_XAI\\\\Experiment3(생성별_모델별)\\\\model_history\\\\{generation}_proposed_tabnetEmbedd.pickle', 'wb') as f:\n",
    "        pickle.dump(tabnet_clf, f)\n",
    "    \n",
    "    X_train_torch = torch.from_numpy(X_train.astype(np.float32))\n",
    "    X_test_torch  = torch.from_numpy(X_test.astype(np.float32))\n",
    "    \n",
    "    def extract_tabnet_embeddings(\n",
    "    tabnet_model,\n",
    "    X_tensor,\n",
    "    device=None,\n",
    "    batch_size=5000\n",
    "    ):\n",
    "        if device is None:\n",
    "            device = next(tabnet_model.network.parameters()).device\n",
    "        \n",
    "        tabnet_model.network.eval()\n",
    "        tabnet_model.network.to(device)\n",
    "        X_tensor = X_tensor.to(device)\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for (xb,) in loader:\n",
    "                embeddings, _ = tabnet_model.network.forward_masks(xb)\n",
    "                        \n",
    "                if isinstance(embeddings, dict):\n",
    "                    if 'embeddings' in embeddings:\n",
    "                        final_emb = embeddings['embeddings'][-1]\n",
    "                    else:\n",
    "                        final_emb = list(embeddings.values())[-1]\n",
    "                else:\n",
    "                    # embeddings가 리스트인 경우, 마지막 decision step의 embedding 사용\n",
    "                    if isinstance(embeddings, list):\n",
    "                        final_emb = embeddings[-1]\n",
    "                    else:\n",
    "                        final_emb = embeddings\n",
    "                \n",
    "                # final_emb의 shape 확인\n",
    "                print(\"Shape of final_emb:\", final_emb.shape)\n",
    "                \n",
    "                all_embeddings.append(final_emb.cpu().numpy())\n",
    "        \n",
    "        final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "        # 최종 shape 출력\n",
    "        print(\"Final embeddings shape:\", final_embeddings.shape)\n",
    "        return final_embeddings\n",
    "    \n",
    "    \n",
    "    # 사용 예시\n",
    "    X_train_feature_attrs = extract_tabnet_embeddings(\n",
    "        tabnet_clf,\n",
    "        X_train_torch,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    X_test_feature_attrs = extract_tabnet_embeddings(\n",
    "        tabnet_clf,\n",
    "        X_test_torch,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    # StandardScaler 객체를 생성합니다.\n",
    "    standard_scaler = StandardScaler()\n",
    "\n",
    "    # fit_transform()을 사용해서 학습과 스케일링을 한 번에 적용합니다.\n",
    "    X_train_feature_attrs_standard = standard_scaler.fit_transform(X_train_feature_attrs)\n",
    "    # 표준화가 완료된 데이터를 데이터프레임 형태로 변환합니다.\n",
    "    X_test_feature_attrs_standard = standard_scaler.transform(X_test_feature_attrs)\n",
    "\n",
    "    with open(f'C:\\\\jupyter\\\\Elementary-student-Weight\\\\GAN_XAI\\\\Experiment3(생성별_모델별)\\\\model_history\\\\{generation}_proposed_standard_scaler.pickle', 'wb') as f:\n",
    "        pickle.dump(standard_scaler, f)\n",
    "\n",
    "    def objective_xgboost(trial, X_train, X_test, y_train, y_test):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "            'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        return accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # 모델 최적화 및 비교\n",
    "    def optimize_and_compare_models(X_train, X_test, y_train, y_test, n_trials=100):\n",
    "        results = {}\n",
    "        \n",
    "        # XGBoost 최적화\n",
    "        study_xgb = optuna.create_study(direction='maximize')\n",
    "        study_xgb.optimize(lambda trial: objective_xgboost(trial, X_train, X_test, y_train, y_test), \n",
    "                        n_trials=n_trials)\n",
    "\n",
    "        \n",
    "        # 최적의 모델 생성 및 결과 저장\n",
    "        # XGBoost\n",
    "        best_xgb = XGBClassifier(**study_xgb.best_params, random_state=42)\n",
    "        best_xgb.fit(X_train, y_train)\n",
    "        xgb_pred = best_xgb.predict(X_test)\n",
    "        results['XGBoost'] = {\n",
    "            'accuracy': accuracy_score(y_test, xgb_pred),\n",
    "            'best_params': study_xgb.best_params\n",
    "        }\n",
    "        return results,xgb_pred,best_xgb\n",
    "\n",
    "    # 모델 실행 및 결과 출력\n",
    "    results ,xgb_pred,best_xgb= optimize_and_compare_models(X_train_feature_attrs_standard, X_test_feature_attrs_standard, \n",
    "                                        y_train.astype(int), y_test.astype(int), n_trials=100)\n",
    "    \n",
    "    \n",
    "    # 결과 출력\n",
    "    for model_name, model_results in results.items():\n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(f\"Best Accuracy: {model_results['accuracy']:.4f}\")\n",
    "        print(\"Best Parameters:\")\n",
    "        for param, value in model_results['best_params'].items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "\n",
    "    \n",
    "    with open(f'C:\\\\jupyter\\\\Elementary-student-Weight\\\\GAN_XAI\\\\Experiment3(생성별_모델별)\\\\model_history\\\\{generation}_proposed.pickle', 'wb') as f:\n",
    "        pickle.dump(best_xgb, f)\n",
    "    \n",
    "    # 각 메트릭 계산\n",
    "    accuracy = accuracy_score(y_test, xgb_pred)\n",
    "    f1 = f1_score(y_test, xgb_pred, average='weighted')\n",
    "    precision, recall = sk(y_test, xgb_pred, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-proposed\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    model_results_list.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_proposed = pd.DataFrame(model_results_list)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jm_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
