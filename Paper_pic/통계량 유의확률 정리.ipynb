{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Comparison models Test Accuracy Test F1-Score Test Precision Test Recall External Accuracy External F1-Score External Precision External Recall\n",
      "Decision Tree vs. Proposed XAI model        p<0.05       p=0.061        p=0.098      p<0.05           p=0.669           p=0.652            p=0.662         p=0.669\n",
      "         GAMs vs. Proposed XAI model        p<0.05        p<0.01         p<0.05      p<0.05           p=0.815           p=0.794            p=0.186         p=0.815\n",
      "          LDA vs. Proposed XAI model        p<0.01        p<0.01         p<0.01     p<0.001           p=0.179           p=0.201            p=0.608         p=0.179\n",
      "  Naive Bayes vs. Proposed XAI model        p<0.05        p<0.05         p<0.05      p<0.05           p=0.462           p=0.531            p=0.309         p=0.462\n",
      "       TabNet vs. Proposed XAI model        p<0.05        p<0.05         p<0.05     p=0.055           p=0.580           p=0.431            p=0.365         p=0.580\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def extract_paired_data():\n",
    "    generation_data = {\n",
    "        'Smote': {\n",
    "            'GAMs': [0.531, 0.511, 0.515, 0.513, 0.364, 0.472, 0.789, 0.364],\n",
    "            'LDA': [0.652, 0.523, 0.511, 0.652, 0.340, 0.445, 0.817, 0.340], \n",
    "            'Naive Bayes': [0.656, 0.582, 0.582, 0.656, 0.316, 0.430, 0.769, 0.316],\n",
    "            'Decision Tree': [0.663, 0.592, 0.536, 0.663, 0.329, 0.431, 0.790, 0.329],\n",
    "            'TabNet': [0.890, 0.855, 0.890, 0.851, 0.702, 0.744, 0.793, 0.702],\n",
    "            'Proposed XAI model': [0.955, 0.940, 0.954, 0.927, 0.749, 0.771, 0.798, 0.749]\n",
    "        },\n",
    "        'Adasyn': {\n",
    "            'GAMs': [0.536, 0.533, 0.540, 0.536, 0.373, 0.480, 0.803, 0.373],\n",
    "            'LDA': [0.644, 0.520, 0.508, 0.644, 0.328, 0.446, 0.827, 0.328],\n",
    "            'Naive Bayes': [0.658, 0.587, 0.597, 0.658, 0.444, 0.556, 0.792, 0.444], \n",
    "            'Decision Tree': [0.658, 0.575, 0.521, 0.658, 0.336, 0.436, 0.800, 0.336],\n",
    "            'TabNet': [0.858, 0.817, 0.824, 0.810, 0.694, 0.734, 0.781, 0.694],\n",
    "            'Proposed XAI model': [0.899, 0.855, 0.902, 0.823, 0.738, 0.760, 0.785, 0.738]\n",
    "        },\n",
    "        'CTGAN': {\n",
    "            'LDA': [0.683, 0.620, 0.648, 0.683, 0.541, 0.626, 0.784, 0.541],\n",
    "            'Naive Bayes': [0.693, 0.678, 0.682, 0.693, 0.571, 0.656, 0.786, 0.571],\n",
    "            'GAMs': [0.722, 0.685, 0.705, 0.722, 0.807, 0.799, 0.791, 0.807],\n",
    "            'Decision Tree': [0.803, 0.813, 0.830, 0.803, 0.668, 0.721, 0.789, 0.668],\n",
    "            'TabNet': [0.856, 0.760, 0.755, 0.767, 0.476, 0.575, 0.778, 0.476],\n",
    "            'Proposed XAI model': [0.869, 0.867, 0.866, 0.869, 0.405, 0.513, 0.793, 0.405]\n",
    "        },\n",
    "        'CopulaGANSynthesizer': {\n",
    "            'Naive Bayes': [0.679, 0.623, 0.627, 0.679, 0.662, 0.717, 0.786, 0.662],\n",
    "            'LDA': [0.686, 0.594, 0.617, 0.686, 0.532, 0.624, 0.778, 0.532],\n",
    "            'GAMs': [0.712, 0.660, 0.710, 0.714, 0.795, 0.790, 0.847, 0.795],\n",
    "            'Decision Tree': [0.778, 0.789, 0.806, 0.778, 0.778, 0.786, 0.796, 0.778],\n",
    "            'TabNet': [0.793, 0.678, 0.699, 0.663, 0.620, 0.692, 0.794, 0.620],\n",
    "            'Proposed XAI model': [0.851, 0.847, 0.845, 0.851, 0.477, 0.582, 0.803, 0.477]\n",
    "        },\n",
    "        'Nbsynthetic': {\n",
    "            'GAMs': [0.789, 0.772, 0.778, 0.789, 0.672, 0.730, 0.806, 0.672],\n",
    "            'LDA': [0.826, 0.801, 0.800, 0.826, 0.506, 0.600, 0.786, 0.506],\n",
    "            'Decision Tree': [0.909, 0.906, 0.918, 0.909, 0.753, 0.773, 0.798, 0.753],\n",
    "            'Naive Bayes': [0.940, 0.940, 0.942, 0.940, 0.730, 0.757, 0.787, 0.730],\n",
    "            'TabNet': [0.970, 0.962, 0.978, 0.947, 0.840, 0.813, 0.792, 0.840],\n",
    "            'Proposed XAI model': [0.980, 0.979, 0.979, 0.980, 0.852, 0.814, 0.781, 0.852]\n",
    "        }\n",
    "    }\n",
    "    return generation_data\n",
    "\n",
    "def perform_comprehensive_ttest():\n",
    "    generation_data = extract_paired_data()\n",
    "    metrics = ['Accuracy', 'F1-Score', 'Precision', 'Recall', \n",
    "               'Accuracy', 'F1-Score', 'Precision', 'Recall']\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    all_models = set()\n",
    "    for gen_data in generation_data.values():\n",
    "        all_models.update(gen_data.keys())\n",
    "    all_models.discard('Proposed XAI model')\n",
    "    all_models = sorted(list(all_models))\n",
    "    \n",
    "    for model in all_models:\n",
    "        row_data = {'Comparison models': f'{model} vs. Proposed XAI model'}\n",
    "        \n",
    "        for metric_idx, metric in enumerate(metrics):\n",
    "            proposed_scores = []\n",
    "            other_scores = []\n",
    "            \n",
    "            for gen_method, models in generation_data.items():\n",
    "                if model in models and 'Proposed XAI model' in models:\n",
    "                    proposed_scores.append(models['Proposed XAI model'][metric_idx])\n",
    "                    other_scores.append(models[model][metric_idx])\n",
    "            \n",
    "            if len(proposed_scores) >= 2:\n",
    "                t_stat, p_value = stats.ttest_rel(proposed_scores, other_scores)\n",
    "                \n",
    "                if p_value < 0.001:\n",
    "                    p_str = 'p<0.001'\n",
    "                elif p_value < 0.01:\n",
    "                    p_str = 'p<0.01'\n",
    "                elif p_value < 0.05:\n",
    "                    p_str = 'p<0.05'\n",
    "                else:\n",
    "                    p_str = f'p={p_value:.3f}'\n",
    "                \n",
    "                if metric_idx < 4:\n",
    "                    column_name = f'Test {metric}'\n",
    "                else:\n",
    "                    column_name = f'External {metric}'\n",
    "                \n",
    "                row_data[column_name] = p_str\n",
    "            else:\n",
    "                if metric_idx < 4:\n",
    "                    column_name = f'Test {metric}'\n",
    "                else:\n",
    "                    column_name = f'External {metric}'\n",
    "                row_data[column_name] = 'N/A'\n",
    "        \n",
    "        comparison_results.append(row_data)\n",
    "    \n",
    "    return pd.DataFrame(comparison_results)\n",
    "\n",
    "# 결과 테이블 생성 및 출력\n",
    "comparison_df = perform_comprehensive_ttest()\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Comparison models Accuracy F1-Score Precision  Recall\n",
      "Decision Tree vs. Proposed XAI model   p<0.05  p=0.061   p=0.098  p<0.05\n",
      "         GAMs vs. Proposed XAI model   p<0.05   p<0.01    p<0.05  p<0.05\n",
      "          LDA vs. Proposed XAI model   p<0.01   p<0.01    p<0.01 p<0.001\n",
      "  Naive Bayes vs. Proposed XAI model   p<0.05   p<0.05    p<0.05  p<0.05\n",
      "       TabNet vs. Proposed XAI model   p<0.05   p<0.05    p<0.05 p=0.055\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def extract_paired_data():\n",
    "    generation_data = {\n",
    "        'Smote': {\n",
    "            'GAMs': [0.531, 0.511, 0.515, 0.513, 0.364, 0.472, 0.789, 0.364],\n",
    "            'LDA': [0.652, 0.523, 0.511, 0.652, 0.340, 0.445, 0.817, 0.340], \n",
    "            'Naive Bayes': [0.656, 0.582, 0.582, 0.656, 0.316, 0.430, 0.769, 0.316],\n",
    "            'Decision Tree': [0.663, 0.592, 0.536, 0.663, 0.329, 0.431, 0.790, 0.329],\n",
    "            'TabNet': [0.890, 0.855, 0.890, 0.851, 0.702, 0.744, 0.793, 0.702],\n",
    "            'Proposed XAI model': [0.955, 0.940, 0.954, 0.927, 0.749, 0.771, 0.798, 0.749]\n",
    "        },\n",
    "        'Adasyn': {\n",
    "            'GAMs': [0.536, 0.533, 0.540, 0.536, 0.373, 0.480, 0.803, 0.373],\n",
    "            'LDA': [0.644, 0.520, 0.508, 0.644, 0.328, 0.446, 0.827, 0.328],\n",
    "            'Naive Bayes': [0.658, 0.587, 0.597, 0.658, 0.444, 0.556, 0.792, 0.444], \n",
    "            'Decision Tree': [0.658, 0.575, 0.521, 0.658, 0.336, 0.436, 0.800, 0.336],\n",
    "            'TabNet': [0.858, 0.817, 0.824, 0.810, 0.694, 0.734, 0.781, 0.694],\n",
    "            'Proposed XAI model': [0.899, 0.855, 0.902, 0.823, 0.738, 0.760, 0.785, 0.738]\n",
    "        },\n",
    "        'CTGAN': {\n",
    "            'LDA': [0.683, 0.620, 0.648, 0.683, 0.541, 0.626, 0.784, 0.541],\n",
    "            'Naive Bayes': [0.693, 0.678, 0.682, 0.693, 0.571, 0.656, 0.786, 0.571],\n",
    "            'GAMs': [0.722, 0.685, 0.705, 0.722, 0.807, 0.799, 0.791, 0.807],\n",
    "            'Decision Tree': [0.803, 0.813, 0.830, 0.803, 0.668, 0.721, 0.789, 0.668],\n",
    "            'TabNet': [0.856, 0.760, 0.755, 0.767, 0.476, 0.575, 0.778, 0.476],\n",
    "            'Proposed XAI model': [0.869, 0.867, 0.866, 0.869, 0.405, 0.513, 0.793, 0.405]\n",
    "        },\n",
    "        'CopulaGANSynthesizer': {\n",
    "            'Naive Bayes': [0.679, 0.623, 0.627, 0.679, 0.662, 0.717, 0.786, 0.662],\n",
    "            'LDA': [0.686, 0.594, 0.617, 0.686, 0.532, 0.624, 0.778, 0.532],\n",
    "            'GAMs': [0.712, 0.660, 0.710, 0.714, 0.795, 0.790, 0.847, 0.795],\n",
    "            'Decision Tree': [0.778, 0.789, 0.806, 0.778, 0.778, 0.786, 0.796, 0.778],\n",
    "            'TabNet': [0.793, 0.678, 0.699, 0.663, 0.620, 0.692, 0.794, 0.620],\n",
    "            'Proposed XAI model': [0.851, 0.847, 0.845, 0.851, 0.477, 0.582, 0.803, 0.477]\n",
    "        },\n",
    "        'Nbsynthetic': {\n",
    "            'GAMs': [0.789, 0.772, 0.778, 0.789, 0.672, 0.730, 0.806, 0.672],\n",
    "            'LDA': [0.826, 0.801, 0.800, 0.826, 0.506, 0.600, 0.786, 0.506],\n",
    "            'Decision Tree': [0.909, 0.906, 0.918, 0.909, 0.753, 0.773, 0.798, 0.753],\n",
    "            'Naive Bayes': [0.940, 0.940, 0.942, 0.940, 0.730, 0.757, 0.787, 0.730],\n",
    "            'TabNet': [0.970, 0.962, 0.978, 0.947, 0.840, 0.813, 0.792, 0.840],\n",
    "            'Proposed XAI model': [0.980, 0.979, 0.979, 0.980, 0.852, 0.814, 0.781, 0.852]\n",
    "        }\n",
    "    }\n",
    "    return generation_data\n",
    "\n",
    "def perform_comprehensive_ttest():\n",
    "    generation_data = extract_paired_data()\n",
    "    metrics = ['Accuracy', 'F1-Score', 'Precision', 'Recall']\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    all_models = set()\n",
    "    for gen_data in generation_data.values():\n",
    "        all_models.update(gen_data.keys())\n",
    "    all_models.discard('Proposed XAI model')\n",
    "    all_models = sorted(list(all_models))\n",
    "    \n",
    "    for model in all_models:\n",
    "        row_data = {'Comparison models': f'{model} vs. Proposed XAI model'}\n",
    "        \n",
    "        for metric_idx, metric in enumerate(metrics):\n",
    "            proposed_scores = []\n",
    "            other_scores = []\n",
    "            \n",
    "            for gen_method, models in generation_data.items():\n",
    "                if model in models and 'Proposed XAI model' in models:\n",
    "                    proposed_scores.append(models['Proposed XAI model'][metric_idx])\n",
    "                    other_scores.append(models[model][metric_idx])\n",
    "            \n",
    "            if len(proposed_scores) >= 2:\n",
    "                t_stat, p_value = stats.ttest_rel(proposed_scores, other_scores)\n",
    "                \n",
    "                if p_value < 0.001:\n",
    "                    p_str = 'p<0.001'\n",
    "                elif p_value < 0.01:\n",
    "                    p_str = 'p<0.01'\n",
    "                elif p_value < 0.05:\n",
    "                    p_str = 'p<0.05'\n",
    "                else:\n",
    "                    p_str = f'p={p_value:.3f}'\n",
    "                \n",
    "                row_data[metric] = p_str\n",
    "            else:\n",
    "                row_data[metric] = 'N/A'\n",
    "        \n",
    "        comparison_results.append(row_data)\n",
    "    \n",
    "    return pd.DataFrame(comparison_results)\n",
    "\n",
    "# 결과 테이블 생성 및 출력\n",
    "comparison_df = perform_comprehensive_ttest()\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jm_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
