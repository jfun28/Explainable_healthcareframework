{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 설명가능한 모델별로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 방법으로 합성한 데이터와 여러개의 XAI 모델을 조합하여 경우의 수 모델학습   \n",
    "합성데이터(5가지): Nbsynthetic(Wgan), CopulaGAN, CTGAN, Smote, ADASYN  \n",
    "모델: TabNet, Naive Bayes, Decision Tree, GAMs, LDA  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델별 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제안한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support as sk\n",
    "from sklearn.metrics import f1_score ## F1 Score 구하기\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "# 로컬 모듈 임포트\n",
    "from data_preparation import preprocess_data, convert_to_tensor\n",
    "from encoding_tabnet import train_tabnet_classifier, extract_tabnet_embeddings\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score\n",
    "from utils import standardize_features, calculate_metrics, save_model, print_results\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 프로젝트 루트 디렉토리 경로 (현재 파일의 상위 디렉토리)\n",
    "path = os.path.abspath('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\jupyter\\\\Explainable Healthcare framework\\\\Explainable_healthcareframework\\\\3_2.Training_XAI_VariousModel'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generation_list = ['smote','adasyn','copulagan','ctgan','nbsynthetic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: smote\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "y_train [0 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.16329 | val_0_accuracy: 0.55407 |  0:00:08s\n",
      "epoch 1  | loss: 0.81139 | val_0_accuracy: 0.3818  |  0:00:16s\n",
      "epoch 2  | loss: 0.76674 | val_0_accuracy: 0.16733 |  0:00:24s\n",
      "epoch 3  | loss: 0.75148 | val_0_accuracy: 0.20227 |  0:00:32s\n",
      "epoch 4  | loss: 0.71359 | val_0_accuracy: 0.27992 |  0:00:40s\n",
      "epoch 5  | loss: 0.68932 | val_0_accuracy: 0.24926 |  0:00:48s\n",
      "epoch 6  | loss: 0.65031 | val_0_accuracy: 0.24596 |  0:00:56s\n",
      "epoch 7  | loss: 0.63668 | val_0_accuracy: 0.33844 |  0:01:04s\n",
      "epoch 8  | loss: 0.62187 | val_0_accuracy: 0.3851  |  0:01:12s\n",
      "epoch 9  | loss: 0.5998  | val_0_accuracy: 0.31569 |  0:01:20s\n",
      "epoch 10 | loss: 0.57584 | val_0_accuracy: 0.30663 |  0:01:28s\n",
      "epoch 11 | loss: 0.57974 | val_0_accuracy: 0.28421 |  0:01:36s\n",
      "epoch 12 | loss: 0.57397 | val_0_accuracy: 0.3818  |  0:01:45s\n",
      "epoch 13 | loss: 0.55295 | val_0_accuracy: 0.47758 |  0:01:53s\n",
      "epoch 14 | loss: 0.52794 | val_0_accuracy: 0.53643 |  0:02:02s\n",
      "epoch 15 | loss: 0.51594 | val_0_accuracy: 0.5511  |  0:02:10s\n",
      "epoch 16 | loss: 0.51061 | val_0_accuracy: 0.5244  |  0:02:18s\n",
      "epoch 17 | loss: 0.51111 | val_0_accuracy: 0.53627 |  0:02:26s\n",
      "epoch 18 | loss: 0.51356 | val_0_accuracy: 0.53808 |  0:02:35s\n",
      "epoch 19 | loss: 0.5106  | val_0_accuracy: 0.5638  |  0:02:43s\n",
      "epoch 20 | loss: 0.51821 | val_0_accuracy: 0.59034 |  0:02:52s\n",
      "epoch 21 | loss: 0.49705 | val_0_accuracy: 0.57006 |  0:03:01s\n",
      "epoch 22 | loss: 0.47446 | val_0_accuracy: 0.57072 |  0:03:10s\n",
      "epoch 23 | loss: 0.46662 | val_0_accuracy: 0.57336 |  0:03:18s\n",
      "epoch 24 | loss: 0.46814 | val_0_accuracy: 0.57072 |  0:03:26s\n",
      "epoch 25 | loss: 0.45136 | val_0_accuracy: 0.58638 |  0:03:34s\n",
      "epoch 26 | loss: 0.4552  | val_0_accuracy: 0.59809 |  0:03:42s\n",
      "epoch 27 | loss: 0.45011 | val_0_accuracy: 0.60023 |  0:03:50s\n",
      "epoch 28 | loss: 0.43452 | val_0_accuracy: 0.62479 |  0:03:57s\n",
      "epoch 29 | loss: 0.44378 | val_0_accuracy: 0.63584 |  0:04:05s\n",
      "epoch 30 | loss: 0.42839 | val_0_accuracy: 0.64013 |  0:04:13s\n",
      "epoch 31 | loss: 0.41275 | val_0_accuracy: 0.64441 |  0:04:22s\n",
      "epoch 32 | loss: 0.4145  | val_0_accuracy: 0.6543  |  0:04:30s\n",
      "epoch 33 | loss: 0.40014 | val_0_accuracy: 0.65793 |  0:04:38s\n",
      "epoch 34 | loss: 0.38789 | val_0_accuracy: 0.69074 |  0:04:46s\n",
      "epoch 35 | loss: 0.38355 | val_0_accuracy: 0.70096 |  0:04:53s\n",
      "epoch 36 | loss: 0.37248 | val_0_accuracy: 0.69832 |  0:05:01s\n",
      "epoch 37 | loss: 0.37752 | val_0_accuracy: 0.70392 |  0:05:09s\n",
      "epoch 38 | loss: 0.40248 | val_0_accuracy: 0.7125  |  0:05:17s\n",
      "epoch 39 | loss: 0.38303 | val_0_accuracy: 0.71431 |  0:05:25s\n",
      "epoch 40 | loss: 0.37043 | val_0_accuracy: 0.71876 |  0:05:33s\n",
      "epoch 41 | loss: 0.36827 | val_0_accuracy: 0.72486 |  0:05:40s\n",
      "epoch 42 | loss: 0.36153 | val_0_accuracy: 0.75437 |  0:05:48s\n",
      "epoch 43 | loss: 0.38122 | val_0_accuracy: 0.71958 |  0:05:56s\n",
      "epoch 44 | loss: 0.3969  | val_0_accuracy: 0.7397  |  0:06:05s\n",
      "epoch 45 | loss: 0.38508 | val_0_accuracy: 0.76607 |  0:06:13s\n",
      "epoch 46 | loss: 0.38115 | val_0_accuracy: 0.75256 |  0:06:21s\n",
      "epoch 47 | loss: 0.35942 | val_0_accuracy: 0.77844 |  0:06:30s\n",
      "epoch 48 | loss: 0.34448 | val_0_accuracy: 0.77316 |  0:06:38s\n",
      "epoch 49 | loss: 0.3407  | val_0_accuracy: 0.80498 |  0:06:46s\n",
      "epoch 50 | loss: 0.34837 | val_0_accuracy: 0.8119  |  0:06:55s\n",
      "epoch 51 | loss: 0.3421  | val_0_accuracy: 0.79113 |  0:07:02s\n",
      "epoch 52 | loss: 0.33273 | val_0_accuracy: 0.82163 |  0:07:10s\n",
      "epoch 53 | loss: 0.32772 | val_0_accuracy: 0.82377 |  0:07:19s\n",
      "epoch 54 | loss: 0.31192 | val_0_accuracy: 0.83465 |  0:07:26s\n",
      "epoch 55 | loss: 0.31067 | val_0_accuracy: 0.82097 |  0:07:34s\n",
      "epoch 56 | loss: 0.30922 | val_0_accuracy: 0.84636 |  0:07:42s\n",
      "epoch 57 | loss: 0.31446 | val_0_accuracy: 0.83729 |  0:07:50s\n",
      "epoch 58 | loss: 0.32021 | val_0_accuracy: 0.83465 |  0:07:58s\n",
      "epoch 59 | loss: 0.30483 | val_0_accuracy: 0.83844 |  0:08:06s\n",
      "epoch 60 | loss: 0.28757 | val_0_accuracy: 0.86647 |  0:08:13s\n",
      "epoch 61 | loss: 0.29588 | val_0_accuracy: 0.83498 |  0:08:21s\n",
      "epoch 62 | loss: 0.32043 | val_0_accuracy: 0.82427 |  0:08:29s\n",
      "epoch 63 | loss: 0.34771 | val_0_accuracy: 0.82526 |  0:08:37s\n",
      "epoch 64 | loss: 0.3362  | val_0_accuracy: 0.83927 |  0:08:45s\n",
      "epoch 65 | loss: 0.32264 | val_0_accuracy: 0.83152 |  0:08:53s\n",
      "epoch 66 | loss: 0.32497 | val_0_accuracy: 0.82872 |  0:09:01s\n",
      "epoch 67 | loss: 0.30699 | val_0_accuracy: 0.83564 |  0:09:09s\n",
      "epoch 68 | loss: 0.28794 | val_0_accuracy: 0.85658 |  0:09:17s\n",
      "epoch 69 | loss: 0.28183 | val_0_accuracy: 0.85889 |  0:09:24s\n",
      "epoch 70 | loss: 0.27124 | val_0_accuracy: 0.8701  |  0:09:32s\n",
      "epoch 71 | loss: 0.26881 | val_0_accuracy: 0.86235 |  0:09:40s\n",
      "epoch 72 | loss: 0.29537 | val_0_accuracy: 0.84075 |  0:09:48s\n",
      "epoch 73 | loss: 0.30605 | val_0_accuracy: 0.84768 |  0:09:56s\n",
      "epoch 74 | loss: 0.30707 | val_0_accuracy: 0.82493 |  0:10:03s\n",
      "epoch 75 | loss: 0.30362 | val_0_accuracy: 0.84042 |  0:10:11s\n",
      "epoch 76 | loss: 0.30655 | val_0_accuracy: 0.833   |  0:10:19s\n",
      "epoch 77 | loss: 0.28881 | val_0_accuracy: 0.86103 |  0:10:27s\n",
      "epoch 78 | loss: 0.27915 | val_0_accuracy: 0.85147 |  0:10:35s\n",
      "epoch 79 | loss: 0.29802 | val_0_accuracy: 0.83185 |  0:10:43s\n",
      "epoch 80 | loss: 0.30471 | val_0_accuracy: 0.8424  |  0:10:51s\n",
      "epoch 81 | loss: 0.31791 | val_0_accuracy: 0.81058 |  0:10:59s\n",
      "epoch 82 | loss: 0.30619 | val_0_accuracy: 0.82443 |  0:11:07s\n",
      "epoch 83 | loss: 0.28577 | val_0_accuracy: 0.84421 |  0:11:15s\n",
      "epoch 84 | loss: 0.29519 | val_0_accuracy: 0.85592 |  0:11:23s\n",
      "epoch 85 | loss: 0.288   | val_0_accuracy: 0.86317 |  0:11:31s\n",
      "epoch 86 | loss: 0.28626 | val_0_accuracy: 0.85823 |  0:11:40s\n",
      "epoch 87 | loss: 0.29454 | val_0_accuracy: 0.8574  |  0:11:47s\n",
      "epoch 88 | loss: 0.27143 | val_0_accuracy: 0.85279 |  0:11:56s\n",
      "epoch 89 | loss: 0.27558 | val_0_accuracy: 0.84735 |  0:12:04s\n",
      "epoch 90 | loss: 0.27246 | val_0_accuracy: 0.87389 |  0:12:12s\n",
      "epoch 91 | loss: 0.27674 | val_0_accuracy: 0.86301 |  0:12:20s\n",
      "epoch 92 | loss: 0.271   | val_0_accuracy: 0.85509 |  0:12:28s\n",
      "epoch 93 | loss: 0.26738 | val_0_accuracy: 0.87043 |  0:12:36s\n",
      "epoch 94 | loss: 0.26881 | val_0_accuracy: 0.86268 |  0:12:44s\n",
      "epoch 95 | loss: 0.25572 | val_0_accuracy: 0.88015 |  0:12:52s\n",
      "epoch 96 | loss: 0.27245 | val_0_accuracy: 0.85279 |  0:12:59s\n",
      "epoch 97 | loss: 0.28514 | val_0_accuracy: 0.86218 |  0:13:08s\n",
      "epoch 98 | loss: 0.26769 | val_0_accuracy: 0.84669 |  0:13:16s\n",
      "epoch 99 | loss: 0.25049 | val_0_accuracy: 0.87092 |  0:13:24s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_val_0_accuracy = 0.88015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: adasyn\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "y_train [0 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.19828 | val_0_accuracy: 0.56084 |  0:00:08s\n",
      "epoch 1  | loss: 0.84413 | val_0_accuracy: 0.47334 |  0:00:16s\n",
      "epoch 2  | loss: 0.796   | val_0_accuracy: 0.61879 |  0:00:24s\n",
      "epoch 3  | loss: 0.75586 | val_0_accuracy: 0.63216 |  0:00:33s\n",
      "epoch 4  | loss: 0.72122 | val_0_accuracy: 0.62523 |  0:00:41s\n",
      "epoch 5  | loss: 0.71673 | val_0_accuracy: 0.61829 |  0:00:49s\n",
      "epoch 6  | loss: 0.70272 | val_0_accuracy: 0.4882  |  0:00:57s\n",
      "epoch 7  | loss: 0.68137 | val_0_accuracy: 0.47482 |  0:01:05s\n",
      "epoch 8  | loss: 0.66545 | val_0_accuracy: 0.54152 |  0:01:13s\n",
      "epoch 9  | loss: 0.66681 | val_0_accuracy: 0.53872 |  0:01:21s\n",
      "epoch 10 | loss: 0.66208 | val_0_accuracy: 0.53987 |  0:01:29s\n",
      "epoch 11 | loss: 0.64205 | val_0_accuracy: 0.4915  |  0:01:36s\n",
      "epoch 12 | loss: 0.62217 | val_0_accuracy: 0.48209 |  0:01:44s\n",
      "epoch 13 | loss: 0.62606 | val_0_accuracy: 0.51197 |  0:01:52s\n",
      "epoch 14 | loss: 0.61438 | val_0_accuracy: 0.54747 |  0:02:00s\n",
      "epoch 15 | loss: 0.60326 | val_0_accuracy: 0.51956 |  0:02:07s\n",
      "epoch 16 | loss: 0.5854  | val_0_accuracy: 0.53096 |  0:02:15s\n",
      "epoch 17 | loss: 0.5932  | val_0_accuracy: 0.52831 |  0:02:23s\n",
      "epoch 18 | loss: 0.58093 | val_0_accuracy: 0.5123  |  0:02:31s\n",
      "epoch 19 | loss: 0.56518 | val_0_accuracy: 0.52897 |  0:02:39s\n",
      "epoch 20 | loss: 0.57082 | val_0_accuracy: 0.54004 |  0:02:47s\n",
      "epoch 21 | loss: 0.54774 | val_0_accuracy: 0.55836 |  0:02:55s\n",
      "epoch 22 | loss: 0.53441 | val_0_accuracy: 0.54449 |  0:03:03s\n",
      "epoch 23 | loss: 0.53394 | val_0_accuracy: 0.55044 |  0:03:12s\n",
      "epoch 24 | loss: 0.51282 | val_0_accuracy: 0.55225 |  0:03:20s\n",
      "epoch 25 | loss: 0.50181 | val_0_accuracy: 0.59749 |  0:03:28s\n",
      "epoch 26 | loss: 0.48909 | val_0_accuracy: 0.60277 |  0:03:35s\n",
      "epoch 27 | loss: 0.47766 | val_0_accuracy: 0.59039 |  0:03:44s\n",
      "epoch 28 | loss: 0.48315 | val_0_accuracy: 0.60756 |  0:03:53s\n",
      "epoch 29 | loss: 0.47832 | val_0_accuracy: 0.62027 |  0:04:00s\n",
      "epoch 30 | loss: 0.46294 | val_0_accuracy: 0.63761 |  0:04:08s\n",
      "epoch 31 | loss: 0.45406 | val_0_accuracy: 0.65263 |  0:04:16s\n",
      "epoch 32 | loss: 0.44953 | val_0_accuracy: 0.66617 |  0:04:24s\n",
      "epoch 33 | loss: 0.43989 | val_0_accuracy: 0.67558 |  0:04:32s\n",
      "epoch 34 | loss: 0.43145 | val_0_accuracy: 0.69424 |  0:04:40s\n",
      "epoch 35 | loss: 0.50841 | val_0_accuracy: 0.66535 |  0:04:48s\n",
      "epoch 36 | loss: 0.4884  | val_0_accuracy: 0.68631 |  0:04:55s\n",
      "epoch 37 | loss: 0.45914 | val_0_accuracy: 0.67723 |  0:05:03s\n",
      "epoch 38 | loss: 0.44228 | val_0_accuracy: 0.7337  |  0:05:11s\n",
      "epoch 39 | loss: 0.42153 | val_0_accuracy: 0.74228 |  0:05:19s\n",
      "epoch 40 | loss: 0.41141 | val_0_accuracy: 0.7403  |  0:05:27s\n",
      "epoch 41 | loss: 0.41021 | val_0_accuracy: 0.76952 |  0:05:35s\n",
      "epoch 42 | loss: 0.40817 | val_0_accuracy: 0.77035 |  0:05:43s\n",
      "epoch 43 | loss: 0.38742 | val_0_accuracy: 0.78719 |  0:05:51s\n",
      "epoch 44 | loss: 0.38863 | val_0_accuracy: 0.77481 |  0:05:59s\n",
      "epoch 45 | loss: 0.42652 | val_0_accuracy: 0.77233 |  0:06:07s\n",
      "epoch 46 | loss: 0.42757 | val_0_accuracy: 0.78422 |  0:06:15s\n",
      "epoch 47 | loss: 0.4183  | val_0_accuracy: 0.77811 |  0:06:22s\n",
      "epoch 48 | loss: 0.42933 | val_0_accuracy: 0.73584 |  0:06:30s\n",
      "epoch 49 | loss: 0.42102 | val_0_accuracy: 0.76705 |  0:06:38s\n",
      "epoch 50 | loss: 0.40761 | val_0_accuracy: 0.7715  |  0:06:46s\n",
      "epoch 51 | loss: 0.38664 | val_0_accuracy: 0.82334 |  0:06:54s\n",
      "epoch 52 | loss: 0.38466 | val_0_accuracy: 0.7966  |  0:07:02s\n",
      "epoch 53 | loss: 0.38645 | val_0_accuracy: 0.82467 |  0:07:10s\n",
      "epoch 54 | loss: 0.3772  | val_0_accuracy: 0.81426 |  0:07:18s\n",
      "epoch 55 | loss: 0.37861 | val_0_accuracy: 0.82334 |  0:07:26s\n",
      "epoch 56 | loss: 0.37088 | val_0_accuracy: 0.82566 |  0:07:34s\n",
      "epoch 57 | loss: 0.37871 | val_0_accuracy: 0.82467 |  0:07:43s\n",
      "epoch 58 | loss: 0.36273 | val_0_accuracy: 0.80617 |  0:07:51s\n",
      "epoch 59 | loss: 0.3546  | val_0_accuracy: 0.82615 |  0:07:59s\n",
      "epoch 60 | loss: 0.35019 | val_0_accuracy: 0.81327 |  0:08:08s\n",
      "epoch 61 | loss: 0.36976 | val_0_accuracy: 0.82813 |  0:08:16s\n",
      "epoch 62 | loss: 0.36009 | val_0_accuracy: 0.8174  |  0:08:24s\n",
      "epoch 63 | loss: 0.35969 | val_0_accuracy: 0.81294 |  0:08:32s\n",
      "epoch 64 | loss: 0.34356 | val_0_accuracy: 0.81674 |  0:08:41s\n",
      "epoch 65 | loss: 0.33336 | val_0_accuracy: 0.83441 |  0:08:50s\n",
      "epoch 66 | loss: 0.33861 | val_0_accuracy: 0.83589 |  0:08:59s\n",
      "epoch 67 | loss: 0.33377 | val_0_accuracy: 0.83457 |  0:09:08s\n",
      "epoch 68 | loss: 0.33452 | val_0_accuracy: 0.85026 |  0:09:17s\n",
      "epoch 69 | loss: 0.33045 | val_0_accuracy: 0.85521 |  0:09:26s\n",
      "epoch 70 | loss: 0.3394  | val_0_accuracy: 0.83424 |  0:09:34s\n",
      "epoch 71 | loss: 0.33568 | val_0_accuracy: 0.83804 |  0:09:43s\n",
      "epoch 72 | loss: 0.32883 | val_0_accuracy: 0.85422 |  0:09:53s\n",
      "epoch 73 | loss: 0.32535 | val_0_accuracy: 0.85901 |  0:10:02s\n",
      "epoch 74 | loss: 0.32593 | val_0_accuracy: 0.84993 |  0:10:10s\n",
      "epoch 75 | loss: 0.33279 | val_0_accuracy: 0.83094 |  0:10:19s\n",
      "epoch 76 | loss: 0.35168 | val_0_accuracy: 0.80502 |  0:10:28s\n",
      "epoch 77 | loss: 0.35298 | val_0_accuracy: 0.80353 |  0:10:36s\n",
      "epoch 78 | loss: 0.41383 | val_0_accuracy: 0.7753  |  0:10:44s\n",
      "epoch 79 | loss: 0.38768 | val_0_accuracy: 0.76672 |  0:10:53s\n",
      "epoch 80 | loss: 0.37042 | val_0_accuracy: 0.7999  |  0:11:01s\n",
      "epoch 81 | loss: 0.34593 | val_0_accuracy: 0.8207  |  0:11:10s\n",
      "epoch 82 | loss: 0.3487  | val_0_accuracy: 0.83011 |  0:11:18s\n",
      "epoch 83 | loss: 0.34015 | val_0_accuracy: 0.81592 |  0:11:26s\n",
      "epoch 84 | loss: 0.34095 | val_0_accuracy: 0.84398 |  0:11:35s\n",
      "epoch 85 | loss: 0.32779 | val_0_accuracy: 0.84365 |  0:11:44s\n",
      "epoch 86 | loss: 0.31695 | val_0_accuracy: 0.85835 |  0:11:53s\n",
      "epoch 87 | loss: 0.3224  | val_0_accuracy: 0.84481 |  0:12:02s\n",
      "epoch 88 | loss: 0.35335 | val_0_accuracy: 0.83787 |  0:12:10s\n",
      "epoch 89 | loss: 0.34571 | val_0_accuracy: 0.8212  |  0:12:19s\n",
      "epoch 90 | loss: 0.3187  | val_0_accuracy: 0.83276 |  0:12:28s\n",
      "epoch 91 | loss: 0.31724 | val_0_accuracy: 0.83705 |  0:12:36s\n",
      "epoch 92 | loss: 0.30804 | val_0_accuracy: 0.83391 |  0:12:43s\n",
      "epoch 93 | loss: 0.30203 | val_0_accuracy: 0.85125 |  0:12:51s\n",
      "epoch 94 | loss: 0.29228 | val_0_accuracy: 0.86    |  0:12:59s\n",
      "epoch 95 | loss: 0.28473 | val_0_accuracy: 0.86132 |  0:13:07s\n",
      "epoch 96 | loss: 0.28848 | val_0_accuracy: 0.85851 |  0:13:15s\n",
      "epoch 97 | loss: 0.29485 | val_0_accuracy: 0.86495 |  0:13:23s\n",
      "epoch 98 | loss: 0.31117 | val_0_accuracy: 0.86165 |  0:13:31s\n",
      "epoch 99 | loss: 0.295   | val_0_accuracy: 0.85686 |  0:13:39s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_val_0_accuracy = 0.86495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: copulagan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "y_train [0 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.07994 | val_0_accuracy: 0.42763 |  0:00:08s\n",
      "epoch 1  | loss: 0.63945 | val_0_accuracy: 0.17606 |  0:00:15s\n",
      "epoch 2  | loss: 0.58187 | val_0_accuracy: 0.16914 |  0:00:23s\n",
      "epoch 3  | loss: 0.55693 | val_0_accuracy: 0.17507 |  0:00:32s\n",
      "epoch 4  | loss: 0.54425 | val_0_accuracy: 0.20162 |  0:00:40s\n",
      "epoch 5  | loss: 0.527   | val_0_accuracy: 0.20293 |  0:00:48s\n",
      "epoch 6  | loss: 0.51533 | val_0_accuracy: 0.24547 |  0:00:56s\n",
      "epoch 7  | loss: 0.5054  | val_0_accuracy: 0.26195 |  0:01:04s\n",
      "epoch 8  | loss: 0.50459 | val_0_accuracy: 0.21316 |  0:01:12s\n",
      "epoch 9  | loss: 0.49838 | val_0_accuracy: 0.24448 |  0:01:20s\n",
      "epoch 10 | loss: 0.48714 | val_0_accuracy: 0.25767 |  0:01:28s\n",
      "epoch 11 | loss: 0.48428 | val_0_accuracy: 0.2913  |  0:01:36s\n",
      "epoch 12 | loss: 0.48276 | val_0_accuracy: 0.34322 |  0:01:44s\n",
      "epoch 13 | loss: 0.48139 | val_0_accuracy: 0.48648 |  0:01:53s\n",
      "epoch 14 | loss: 0.48281 | val_0_accuracy: 0.48632 |  0:02:01s\n",
      "epoch 15 | loss: 0.48361 | val_0_accuracy: 0.51022 |  0:02:10s\n",
      "epoch 16 | loss: 0.47307 | val_0_accuracy: 0.5788  |  0:02:18s\n",
      "epoch 17 | loss: 0.46677 | val_0_accuracy: 0.55984 |  0:02:26s\n",
      "epoch 18 | loss: 0.46213 | val_0_accuracy: 0.61161 |  0:02:34s\n",
      "epoch 19 | loss: 0.46801 | val_0_accuracy: 0.62661 |  0:02:42s\n",
      "epoch 20 | loss: 0.47473 | val_0_accuracy: 0.68068 |  0:02:50s\n",
      "epoch 21 | loss: 0.46887 | val_0_accuracy: 0.58886 |  0:02:58s\n",
      "epoch 22 | loss: 0.46086 | val_0_accuracy: 0.6093  |  0:03:06s\n",
      "epoch 23 | loss: 0.46522 | val_0_accuracy: 0.61194 |  0:03:14s\n",
      "epoch 24 | loss: 0.46439 | val_0_accuracy: 0.72799 |  0:03:22s\n",
      "epoch 25 | loss: 0.46241 | val_0_accuracy: 0.71909 |  0:03:31s\n",
      "epoch 26 | loss: 0.45699 | val_0_accuracy: 0.74415 |  0:03:39s\n",
      "epoch 27 | loss: 0.45118 | val_0_accuracy: 0.70162 |  0:03:47s\n",
      "epoch 28 | loss: 0.44913 | val_0_accuracy: 0.71316 |  0:03:55s\n",
      "epoch 29 | loss: 0.4432  | val_0_accuracy: 0.73755 |  0:04:03s\n",
      "epoch 30 | loss: 0.44651 | val_0_accuracy: 0.74398 |  0:04:11s\n",
      "epoch 31 | loss: 0.4469  | val_0_accuracy: 0.75091 |  0:04:19s\n",
      "epoch 32 | loss: 0.45127 | val_0_accuracy: 0.76063 |  0:04:28s\n",
      "epoch 33 | loss: 0.44977 | val_0_accuracy: 0.7664  |  0:04:36s\n",
      "epoch 34 | loss: 0.4448  | val_0_accuracy: 0.77877 |  0:04:43s\n",
      "epoch 35 | loss: 0.43979 | val_0_accuracy: 0.76492 |  0:04:52s\n",
      "epoch 36 | loss: 0.43601 | val_0_accuracy: 0.77778 |  0:04:59s\n",
      "epoch 37 | loss: 0.42803 | val_0_accuracy: 0.79542 |  0:05:08s\n",
      "epoch 38 | loss: 0.43016 | val_0_accuracy: 0.79344 |  0:05:16s\n",
      "epoch 39 | loss: 0.42942 | val_0_accuracy: 0.7936  |  0:05:24s\n",
      "epoch 40 | loss: 0.4307  | val_0_accuracy: 0.78701 |  0:05:32s\n",
      "epoch 41 | loss: 0.4285  | val_0_accuracy: 0.80201 |  0:05:40s\n",
      "epoch 42 | loss: 0.43964 | val_0_accuracy: 0.79542 |  0:05:48s\n",
      "epoch 43 | loss: 0.4312  | val_0_accuracy: 0.82146 |  0:05:56s\n",
      "epoch 44 | loss: 0.4286  | val_0_accuracy: 0.81388 |  0:06:04s\n",
      "epoch 45 | loss: 0.42923 | val_0_accuracy: 0.82822 |  0:06:12s\n",
      "epoch 46 | loss: 0.42592 | val_0_accuracy: 0.82624 |  0:06:20s\n",
      "epoch 47 | loss: 0.42638 | val_0_accuracy: 0.81075 |  0:06:28s\n",
      "epoch 48 | loss: 0.418   | val_0_accuracy: 0.81503 |  0:06:37s\n",
      "epoch 49 | loss: 0.41501 | val_0_accuracy: 0.82608 |  0:06:45s\n",
      "epoch 50 | loss: 0.41301 | val_0_accuracy: 0.81685 |  0:06:52s\n",
      "epoch 51 | loss: 0.41202 | val_0_accuracy: 0.82443 |  0:07:01s\n",
      "epoch 52 | loss: 0.40717 | val_0_accuracy: 0.82624 |  0:07:09s\n",
      "epoch 53 | loss: 0.40432 | val_0_accuracy: 0.83597 |  0:07:17s\n",
      "epoch 54 | loss: 0.40552 | val_0_accuracy: 0.8307  |  0:07:25s\n",
      "epoch 55 | loss: 0.39689 | val_0_accuracy: 0.83103 |  0:07:33s\n",
      "epoch 56 | loss: 0.39906 | val_0_accuracy: 0.82756 |  0:07:41s\n",
      "epoch 57 | loss: 0.39503 | val_0_accuracy: 0.83037 |  0:07:49s\n",
      "epoch 58 | loss: 0.40657 | val_0_accuracy: 0.80333 |  0:07:58s\n",
      "epoch 59 | loss: 0.39783 | val_0_accuracy: 0.8147  |  0:08:05s\n",
      "epoch 60 | loss: 0.39965 | val_0_accuracy: 0.7087  |  0:08:13s\n",
      "epoch 61 | loss: 0.39019 | val_0_accuracy: 0.76426 |  0:08:21s\n",
      "epoch 62 | loss: 0.39682 | val_0_accuracy: 0.75816 |  0:08:29s\n",
      "epoch 63 | loss: 0.38771 | val_0_accuracy: 0.68513 |  0:08:37s\n",
      "epoch 64 | loss: 0.38374 | val_0_accuracy: 0.82806 |  0:08:46s\n",
      "epoch 65 | loss: 0.38845 | val_0_accuracy: 0.80762 |  0:08:54s\n",
      "epoch 66 | loss: 0.39469 | val_0_accuracy: 0.76294 |  0:09:02s\n",
      "epoch 67 | loss: 0.3791  | val_0_accuracy: 0.7542  |  0:09:10s\n",
      "epoch 68 | loss: 0.37715 | val_0_accuracy: 0.84141 |  0:09:18s\n",
      "epoch 69 | loss: 0.37064 | val_0_accuracy: 0.78849 |  0:09:26s\n",
      "epoch 70 | loss: 0.36629 | val_0_accuracy: 0.79855 |  0:09:34s\n",
      "epoch 71 | loss: 0.36105 | val_0_accuracy: 0.82476 |  0:09:42s\n",
      "epoch 72 | loss: 0.36102 | val_0_accuracy: 0.84454 |  0:09:50s\n",
      "epoch 73 | loss: 0.36314 | val_0_accuracy: 0.78421 |  0:09:58s\n",
      "epoch 74 | loss: 0.3758  | val_0_accuracy: 0.85361 |  0:10:06s\n",
      "epoch 75 | loss: 0.36463 | val_0_accuracy: 0.85625 |  0:10:14s\n",
      "epoch 76 | loss: 0.36464 | val_0_accuracy: 0.84174 |  0:10:22s\n",
      "epoch 77 | loss: 0.37513 | val_0_accuracy: 0.83762 |  0:10:30s\n",
      "epoch 78 | loss: 0.36603 | val_0_accuracy: 0.76492 |  0:10:38s\n",
      "epoch 79 | loss: 0.37526 | val_0_accuracy: 0.72717 |  0:10:46s\n",
      "epoch 80 | loss: 0.36989 | val_0_accuracy: 0.83647 |  0:10:54s\n",
      "epoch 81 | loss: 0.35531 | val_0_accuracy: 0.67293 |  0:11:03s\n",
      "epoch 82 | loss: 0.34482 | val_0_accuracy: 0.7697  |  0:11:11s\n",
      "epoch 83 | loss: 0.34021 | val_0_accuracy: 0.67128 |  0:11:20s\n",
      "epoch 84 | loss: 0.33841 | val_0_accuracy: 0.59314 |  0:11:28s\n",
      "epoch 85 | loss: 0.34169 | val_0_accuracy: 0.82245 |  0:11:36s\n",
      "epoch 86 | loss: 0.3453  | val_0_accuracy: 0.84718 |  0:11:44s\n",
      "epoch 87 | loss: 0.34432 | val_0_accuracy: 0.69041 |  0:11:52s\n",
      "epoch 88 | loss: 0.34783 | val_0_accuracy: 0.83778 |  0:12:00s\n",
      "epoch 89 | loss: 0.35223 | val_0_accuracy: 0.85048 |  0:12:08s\n",
      "epoch 90 | loss: 0.34831 | val_0_accuracy: 0.79014 |  0:12:16s\n",
      "epoch 91 | loss: 0.35205 | val_0_accuracy: 0.83119 |  0:12:24s\n",
      "epoch 92 | loss: 0.34404 | val_0_accuracy: 0.82146 |  0:12:32s\n",
      "epoch 93 | loss: 0.34433 | val_0_accuracy: 0.8541  |  0:12:40s\n",
      "epoch 94 | loss: 0.34228 | val_0_accuracy: 0.82954 |  0:12:48s\n",
      "epoch 95 | loss: 0.3418  | val_0_accuracy: 0.85394 |  0:12:55s\n",
      "epoch 96 | loss: 0.33664 | val_0_accuracy: 0.8574  |  0:13:03s\n",
      "epoch 97 | loss: 0.34146 | val_0_accuracy: 0.84092 |  0:13:11s\n",
      "epoch 98 | loss: 0.32578 | val_0_accuracy: 0.80992 |  0:13:19s\n",
      "epoch 99 | loss: 0.32085 | val_0_accuracy: 0.8635  |  0:13:26s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_accuracy = 0.8635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: ctgan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "y_train [0 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.08804 | val_0_accuracy: 0.5605  |  0:00:07s\n",
      "epoch 1  | loss: 0.72309 | val_0_accuracy: 0.4789  |  0:00:15s\n",
      "epoch 2  | loss: 0.63216 | val_0_accuracy: 0.42384 |  0:00:23s\n",
      "epoch 3  | loss: 0.59625 | val_0_accuracy: 0.22931 |  0:00:30s\n",
      "epoch 4  | loss: 0.57025 | val_0_accuracy: 0.19354 |  0:00:38s\n",
      "epoch 5  | loss: 0.55339 | val_0_accuracy: 0.25816 |  0:00:46s\n",
      "epoch 6  | loss: 0.55172 | val_0_accuracy: 0.22667 |  0:00:53s\n",
      "epoch 7  | loss: 0.54337 | val_0_accuracy: 0.18002 |  0:01:01s\n",
      "epoch 8  | loss: 0.53007 | val_0_accuracy: 0.1937  |  0:01:09s\n",
      "epoch 9  | loss: 0.51979 | val_0_accuracy: 0.21217 |  0:01:17s\n",
      "epoch 10 | loss: 0.53505 | val_0_accuracy: 0.35345 |  0:01:24s\n",
      "epoch 11 | loss: 0.52431 | val_0_accuracy: 0.42862 |  0:01:32s\n",
      "epoch 12 | loss: 0.50974 | val_0_accuracy: 0.35724 |  0:01:40s\n",
      "epoch 13 | loss: 0.50298 | val_0_accuracy: 0.32295 |  0:01:47s\n",
      "epoch 14 | loss: 0.49876 | val_0_accuracy: 0.37224 |  0:01:55s\n",
      "epoch 15 | loss: 0.50096 | val_0_accuracy: 0.42499 |  0:02:03s\n",
      "epoch 16 | loss: 0.49675 | val_0_accuracy: 0.44411 |  0:02:10s\n",
      "epoch 17 | loss: 0.48648 | val_0_accuracy: 0.54187 |  0:02:18s\n",
      "epoch 18 | loss: 0.48095 | val_0_accuracy: 0.55391 |  0:02:26s\n",
      "epoch 19 | loss: 0.47694 | val_0_accuracy: 0.59743 |  0:02:33s\n",
      "epoch 20 | loss: 0.46991 | val_0_accuracy: 0.61589 |  0:02:41s\n",
      "epoch 21 | loss: 0.46083 | val_0_accuracy: 0.63749 |  0:02:48s\n",
      "epoch 22 | loss: 0.45294 | val_0_accuracy: 0.68183 |  0:02:56s\n",
      "epoch 23 | loss: 0.45586 | val_0_accuracy: 0.68612 |  0:03:04s\n",
      "epoch 24 | loss: 0.45515 | val_0_accuracy: 0.7092  |  0:03:12s\n",
      "epoch 25 | loss: 0.45881 | val_0_accuracy: 0.70689 |  0:03:20s\n",
      "epoch 26 | loss: 0.45399 | val_0_accuracy: 0.73887 |  0:03:27s\n",
      "epoch 27 | loss: 0.44632 | val_0_accuracy: 0.73277 |  0:03:35s\n",
      "epoch 28 | loss: 0.44843 | val_0_accuracy: 0.74926 |  0:03:43s\n",
      "epoch 29 | loss: 0.44753 | val_0_accuracy: 0.74283 |  0:03:50s\n",
      "epoch 30 | loss: 0.44467 | val_0_accuracy: 0.77432 |  0:03:58s\n",
      "epoch 31 | loss: 0.44335 | val_0_accuracy: 0.7791  |  0:04:06s\n",
      "epoch 32 | loss: 0.44899 | val_0_accuracy: 0.78882 |  0:04:14s\n",
      "epoch 33 | loss: 0.43353 | val_0_accuracy: 0.79723 |  0:04:21s\n",
      "epoch 34 | loss: 0.42876 | val_0_accuracy: 0.80646 |  0:04:29s\n",
      "epoch 35 | loss: 0.42194 | val_0_accuracy: 0.80943 |  0:04:37s\n",
      "epoch 36 | loss: 0.41721 | val_0_accuracy: 0.80465 |  0:04:44s\n",
      "epoch 37 | loss: 0.4205  | val_0_accuracy: 0.81388 |  0:04:52s\n",
      "epoch 38 | loss: 0.42245 | val_0_accuracy: 0.82377 |  0:05:00s\n",
      "epoch 39 | loss: 0.4202  | val_0_accuracy: 0.82509 |  0:05:07s\n",
      "epoch 40 | loss: 0.41037 | val_0_accuracy: 0.82888 |  0:05:15s\n",
      "epoch 41 | loss: 0.40823 | val_0_accuracy: 0.82888 |  0:05:23s\n",
      "epoch 42 | loss: 0.40931 | val_0_accuracy: 0.83201 |  0:05:30s\n",
      "epoch 43 | loss: 0.40708 | val_0_accuracy: 0.83004 |  0:05:38s\n",
      "epoch 44 | loss: 0.40594 | val_0_accuracy: 0.83976 |  0:05:46s\n",
      "epoch 45 | loss: 0.40281 | val_0_accuracy: 0.83811 |  0:05:54s\n",
      "epoch 46 | loss: 0.39829 | val_0_accuracy: 0.83729 |  0:06:01s\n",
      "epoch 47 | loss: 0.39397 | val_0_accuracy: 0.83844 |  0:06:09s\n",
      "epoch 48 | loss: 0.40338 | val_0_accuracy: 0.84471 |  0:06:17s\n",
      "epoch 49 | loss: 0.39431 | val_0_accuracy: 0.84125 |  0:06:24s\n",
      "epoch 50 | loss: 0.39742 | val_0_accuracy: 0.84174 |  0:06:32s\n",
      "epoch 51 | loss: 0.3987  | val_0_accuracy: 0.83086 |  0:06:40s\n",
      "epoch 52 | loss: 0.39632 | val_0_accuracy: 0.84405 |  0:06:47s\n",
      "epoch 53 | loss: 0.38915 | val_0_accuracy: 0.84768 |  0:06:55s\n",
      "epoch 54 | loss: 0.39004 | val_0_accuracy: 0.84784 |  0:07:03s\n",
      "epoch 55 | loss: 0.38763 | val_0_accuracy: 0.8513  |  0:07:10s\n",
      "epoch 56 | loss: 0.38457 | val_0_accuracy: 0.85279 |  0:07:18s\n",
      "epoch 57 | loss: 0.38146 | val_0_accuracy: 0.85262 |  0:07:26s\n",
      "epoch 58 | loss: 0.38276 | val_0_accuracy: 0.84388 |  0:07:34s\n",
      "epoch 59 | loss: 0.38045 | val_0_accuracy: 0.85312 |  0:07:41s\n",
      "epoch 60 | loss: 0.37738 | val_0_accuracy: 0.84586 |  0:07:49s\n",
      "epoch 61 | loss: 0.38596 | val_0_accuracy: 0.84652 |  0:07:57s\n",
      "epoch 62 | loss: 0.39429 | val_0_accuracy: 0.80201 |  0:08:04s\n",
      "epoch 63 | loss: 0.38759 | val_0_accuracy: 0.84454 |  0:08:12s\n",
      "epoch 64 | loss: 0.37866 | val_0_accuracy: 0.84125 |  0:08:20s\n",
      "epoch 65 | loss: 0.37996 | val_0_accuracy: 0.85608 |  0:08:28s\n",
      "epoch 66 | loss: 0.38317 | val_0_accuracy: 0.8213  |  0:08:35s\n",
      "epoch 67 | loss: 0.38895 | val_0_accuracy: 0.84388 |  0:08:43s\n",
      "epoch 68 | loss: 0.38535 | val_0_accuracy: 0.84289 |  0:08:51s\n",
      "epoch 69 | loss: 0.38283 | val_0_accuracy: 0.83828 |  0:08:58s\n",
      "epoch 70 | loss: 0.37686 | val_0_accuracy: 0.84916 |  0:09:06s\n",
      "epoch 71 | loss: 0.37561 | val_0_accuracy: 0.85114 |  0:09:14s\n",
      "epoch 72 | loss: 0.37316 | val_0_accuracy: 0.84718 |  0:09:21s\n",
      "epoch 73 | loss: 0.38081 | val_0_accuracy: 0.8452  |  0:09:29s\n",
      "epoch 74 | loss: 0.38426 | val_0_accuracy: 0.83201 |  0:09:37s\n",
      "epoch 75 | loss: 0.381   | val_0_accuracy: 0.82509 |  0:09:44s\n",
      "epoch 76 | loss: 0.38077 | val_0_accuracy: 0.82608 |  0:09:52s\n",
      "epoch 77 | loss: 0.37907 | val_0_accuracy: 0.83811 |  0:10:00s\n",
      "epoch 78 | loss: 0.37738 | val_0_accuracy: 0.83729 |  0:10:07s\n",
      "epoch 79 | loss: 0.37626 | val_0_accuracy: 0.85345 |  0:10:15s\n",
      "epoch 80 | loss: 0.37822 | val_0_accuracy: 0.84009 |  0:10:22s\n",
      "epoch 81 | loss: 0.3671  | val_0_accuracy: 0.85691 |  0:10:30s\n",
      "epoch 82 | loss: 0.38072 | val_0_accuracy: 0.84405 |  0:10:38s\n",
      "epoch 83 | loss: 0.38272 | val_0_accuracy: 0.82476 |  0:10:45s\n",
      "epoch 84 | loss: 0.37486 | val_0_accuracy: 0.85823 |  0:10:53s\n",
      "epoch 85 | loss: 0.36497 | val_0_accuracy: 0.85608 |  0:11:01s\n",
      "epoch 86 | loss: 0.36829 | val_0_accuracy: 0.84191 |  0:11:08s\n",
      "epoch 87 | loss: 0.36931 | val_0_accuracy: 0.84454 |  0:11:16s\n",
      "epoch 88 | loss: 0.36923 | val_0_accuracy: 0.84784 |  0:11:24s\n",
      "epoch 89 | loss: 0.37454 | val_0_accuracy: 0.8452  |  0:11:31s\n",
      "epoch 90 | loss: 0.37653 | val_0_accuracy: 0.83828 |  0:11:39s\n",
      "epoch 91 | loss: 0.38038 | val_0_accuracy: 0.83663 |  0:11:46s\n",
      "epoch 92 | loss: 0.38652 | val_0_accuracy: 0.80514 |  0:11:54s\n",
      "epoch 93 | loss: 0.38916 | val_0_accuracy: 0.84092 |  0:12:02s\n",
      "epoch 94 | loss: 0.37573 | val_0_accuracy: 0.83251 |  0:12:09s\n",
      "epoch 95 | loss: 0.37615 | val_0_accuracy: 0.83432 |  0:12:17s\n",
      "epoch 96 | loss: 0.3822  | val_0_accuracy: 0.83778 |  0:12:24s\n",
      "epoch 97 | loss: 0.3805  | val_0_accuracy: 0.83168 |  0:12:32s\n",
      "epoch 98 | loss: 0.37492 | val_0_accuracy: 0.84866 |  0:12:40s\n",
      "epoch 99 | loss: 0.36664 | val_0_accuracy: 0.85048 |  0:12:47s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 84 and best_val_0_accuracy = 0.85823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: nbsynthetic\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "y_train [0 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.89167 | val_0_accuracy: 0.48665 |  0:00:07s\n",
      "epoch 1  | loss: 0.35253 | val_0_accuracy: 0.33218 |  0:00:15s\n",
      "epoch 2  | loss: 0.2871  | val_0_accuracy: 0.57715 |  0:00:23s\n",
      "epoch 3  | loss: 0.25711 | val_0_accuracy: 0.26855 |  0:00:30s\n",
      "epoch 4  | loss: 0.23768 | val_0_accuracy: 0.27316 |  0:00:38s\n",
      "epoch 5  | loss: 0.22313 | val_0_accuracy: 0.34289 |  0:00:45s\n",
      "epoch 6  | loss: 0.20462 | val_0_accuracy: 0.35262 |  0:00:53s\n",
      "epoch 7  | loss: 0.19456 | val_0_accuracy: 0.37751 |  0:01:01s\n",
      "epoch 8  | loss: 0.18571 | val_0_accuracy: 0.41642 |  0:01:08s\n",
      "epoch 9  | loss: 0.18175 | val_0_accuracy: 0.38971 |  0:01:16s\n",
      "epoch 10 | loss: 0.17186 | val_0_accuracy: 0.42532 |  0:01:24s\n",
      "epoch 11 | loss: 0.16775 | val_0_accuracy: 0.50082 |  0:01:31s\n",
      "epoch 12 | loss: 0.16077 | val_0_accuracy: 0.58737 |  0:01:39s\n",
      "epoch 13 | loss: 0.16308 | val_0_accuracy: 0.55325 |  0:01:47s\n",
      "epoch 14 | loss: 0.15957 | val_0_accuracy: 0.63683 |  0:01:54s\n",
      "epoch 15 | loss: 0.15936 | val_0_accuracy: 0.70772 |  0:02:02s\n",
      "epoch 16 | loss: 0.15843 | val_0_accuracy: 0.75536 |  0:02:10s\n",
      "epoch 17 | loss: 0.1556  | val_0_accuracy: 0.81091 |  0:02:18s\n",
      "epoch 18 | loss: 0.15041 | val_0_accuracy: 0.80926 |  0:02:25s\n",
      "epoch 19 | loss: 0.14792 | val_0_accuracy: 0.85048 |  0:02:33s\n",
      "epoch 20 | loss: 0.14416 | val_0_accuracy: 0.88065 |  0:02:40s\n",
      "epoch 21 | loss: 0.14229 | val_0_accuracy: 0.88394 |  0:02:48s\n",
      "epoch 22 | loss: 0.14269 | val_0_accuracy: 0.89812 |  0:02:56s\n",
      "epoch 23 | loss: 0.1371  | val_0_accuracy: 0.90917 |  0:03:03s\n",
      "epoch 24 | loss: 0.1516  | val_0_accuracy: 0.92268 |  0:03:11s\n",
      "epoch 25 | loss: 0.15318 | val_0_accuracy: 0.88856 |  0:03:19s\n",
      "epoch 26 | loss: 0.17008 | val_0_accuracy: 0.87257 |  0:03:27s\n",
      "epoch 27 | loss: 0.15864 | val_0_accuracy: 0.88856 |  0:03:34s\n",
      "epoch 28 | loss: 0.14983 | val_0_accuracy: 0.90801 |  0:03:42s\n",
      "epoch 29 | loss: 0.14349 | val_0_accuracy: 0.92334 |  0:03:50s\n",
      "epoch 30 | loss: 0.13888 | val_0_accuracy: 0.93521 |  0:03:57s\n",
      "epoch 31 | loss: 0.13874 | val_0_accuracy: 0.94857 |  0:04:05s\n",
      "epoch 32 | loss: 0.1375  | val_0_accuracy: 0.9545  |  0:04:13s\n",
      "epoch 33 | loss: 0.13787 | val_0_accuracy: 0.94444 |  0:04:20s\n",
      "epoch 34 | loss: 0.14092 | val_0_accuracy: 0.95203 |  0:04:28s\n",
      "epoch 35 | loss: 0.13698 | val_0_accuracy: 0.95796 |  0:04:36s\n",
      "epoch 36 | loss: 0.1373  | val_0_accuracy: 0.95912 |  0:04:43s\n",
      "epoch 37 | loss: 0.13394 | val_0_accuracy: 0.96258 |  0:04:51s\n",
      "epoch 38 | loss: 0.13185 | val_0_accuracy: 0.96258 |  0:04:59s\n",
      "epoch 39 | loss: 0.1286  | val_0_accuracy: 0.96489 |  0:05:06s\n",
      "epoch 40 | loss: 0.12738 | val_0_accuracy: 0.96142 |  0:05:14s\n",
      "epoch 41 | loss: 0.12785 | val_0_accuracy: 0.96423 |  0:05:22s\n",
      "epoch 42 | loss: 0.12827 | val_0_accuracy: 0.96208 |  0:05:29s\n",
      "epoch 43 | loss: 0.1257  | val_0_accuracy: 0.96489 |  0:05:37s\n",
      "epoch 44 | loss: 0.12605 | val_0_accuracy: 0.96241 |  0:05:45s\n",
      "epoch 45 | loss: 0.12418 | val_0_accuracy: 0.96538 |  0:05:52s\n",
      "epoch 46 | loss: 0.12377 | val_0_accuracy: 0.96489 |  0:06:00s\n",
      "epoch 47 | loss: 0.12388 | val_0_accuracy: 0.96307 |  0:06:07s\n",
      "epoch 48 | loss: 0.12215 | val_0_accuracy: 0.96538 |  0:06:15s\n",
      "epoch 49 | loss: 0.12173 | val_0_accuracy: 0.96736 |  0:06:23s\n",
      "epoch 50 | loss: 0.11832 | val_0_accuracy: 0.96686 |  0:06:30s\n",
      "epoch 51 | loss: 0.12152 | val_0_accuracy: 0.96555 |  0:06:38s\n",
      "epoch 52 | loss: 0.12682 | val_0_accuracy: 0.96324 |  0:06:46s\n",
      "epoch 53 | loss: 0.12419 | val_0_accuracy: 0.96505 |  0:06:53s\n",
      "epoch 54 | loss: 0.12002 | val_0_accuracy: 0.9667  |  0:07:01s\n",
      "epoch 55 | loss: 0.11917 | val_0_accuracy: 0.96456 |  0:07:09s\n",
      "epoch 56 | loss: 0.11968 | val_0_accuracy: 0.96769 |  0:07:16s\n",
      "epoch 57 | loss: 0.11885 | val_0_accuracy: 0.96686 |  0:07:24s\n",
      "epoch 58 | loss: 0.12072 | val_0_accuracy: 0.96653 |  0:07:32s\n",
      "epoch 59 | loss: 0.1186  | val_0_accuracy: 0.96769 |  0:07:40s\n",
      "epoch 60 | loss: 0.11823 | val_0_accuracy: 0.9667  |  0:07:47s\n",
      "epoch 61 | loss: 0.11807 | val_0_accuracy: 0.96736 |  0:07:55s\n",
      "epoch 62 | loss: 0.11794 | val_0_accuracy: 0.96703 |  0:08:03s\n",
      "epoch 63 | loss: 0.115   | val_0_accuracy: 0.96934 |  0:08:10s\n",
      "epoch 64 | loss: 0.11424 | val_0_accuracy: 0.96736 |  0:08:18s\n",
      "epoch 65 | loss: 0.11356 | val_0_accuracy: 0.96785 |  0:08:26s\n",
      "epoch 66 | loss: 0.1142  | val_0_accuracy: 0.96571 |  0:08:34s\n",
      "epoch 67 | loss: 0.11248 | val_0_accuracy: 0.96703 |  0:08:41s\n",
      "epoch 68 | loss: 0.11522 | val_0_accuracy: 0.96159 |  0:08:49s\n",
      "epoch 69 | loss: 0.12924 | val_0_accuracy: 0.95384 |  0:08:57s\n",
      "epoch 70 | loss: 0.12507 | val_0_accuracy: 0.96109 |  0:09:04s\n",
      "epoch 71 | loss: 0.12183 | val_0_accuracy: 0.95994 |  0:09:12s\n",
      "epoch 72 | loss: 0.1177  | val_0_accuracy: 0.96011 |  0:09:20s\n",
      "epoch 73 | loss: 0.11487 | val_0_accuracy: 0.96653 |  0:09:27s\n",
      "epoch 74 | loss: 0.11321 | val_0_accuracy: 0.96406 |  0:09:35s\n",
      "epoch 75 | loss: 0.11187 | val_0_accuracy: 0.96703 |  0:09:43s\n",
      "epoch 76 | loss: 0.11245 | val_0_accuracy: 0.96621 |  0:09:50s\n",
      "epoch 77 | loss: 0.11121 | val_0_accuracy: 0.96439 |  0:09:58s\n",
      "epoch 78 | loss: 0.1092  | val_0_accuracy: 0.96653 |  0:10:05s\n",
      "epoch 79 | loss: 0.10788 | val_0_accuracy: 0.96522 |  0:10:13s\n",
      "epoch 80 | loss: 0.10659 | val_0_accuracy: 0.96653 |  0:10:21s\n",
      "epoch 81 | loss: 0.10768 | val_0_accuracy: 0.96769 |  0:10:29s\n",
      "epoch 82 | loss: 0.10856 | val_0_accuracy: 0.96752 |  0:10:36s\n",
      "epoch 83 | loss: 0.1097  | val_0_accuracy: 0.96802 |  0:10:44s\n",
      "epoch 84 | loss: 0.10625 | val_0_accuracy: 0.96835 |  0:10:52s\n",
      "epoch 85 | loss: 0.10747 | val_0_accuracy: 0.96472 |  0:11:00s\n",
      "epoch 86 | loss: 0.10476 | val_0_accuracy: 0.96719 |  0:11:07s\n",
      "epoch 87 | loss: 0.10734 | val_0_accuracy: 0.96769 |  0:11:15s\n",
      "epoch 88 | loss: 0.10939 | val_0_accuracy: 0.96274 |  0:11:22s\n",
      "epoch 89 | loss: 0.1096  | val_0_accuracy: 0.96208 |  0:11:30s\n",
      "epoch 90 | loss: 0.1063  | val_0_accuracy: 0.96604 |  0:11:38s\n",
      "epoch 91 | loss: 0.10537 | val_0_accuracy: 0.96571 |  0:11:45s\n",
      "epoch 92 | loss: 0.10524 | val_0_accuracy: 0.96785 |  0:11:53s\n",
      "epoch 93 | loss: 0.10309 | val_0_accuracy: 0.96851 |  0:12:01s\n",
      "epoch 94 | loss: 0.10504 | val_0_accuracy: 0.96802 |  0:12:08s\n",
      "epoch 95 | loss: 0.1043  | val_0_accuracy: 0.96719 |  0:12:16s\n",
      "epoch 96 | loss: 0.10066 | val_0_accuracy: 0.96637 |  0:12:23s\n",
      "epoch 97 | loss: 0.09975 | val_0_accuracy: 0.96851 |  0:12:31s\n",
      "epoch 98 | loss: 0.10215 | val_0_accuracy: 0.96752 |  0:12:39s\n",
      "epoch 99 | loss: 0.11885 | val_0_accuracy: 0.96373 |  0:12:46s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 63 and best_val_0_accuracy = 0.96934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    print(f\"\\n처리 중인 생성 모델: {generation}\")\n",
    "        \n",
    "    # 1. 데이터 전처리\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = preprocess_data(generation)\n",
    "    \n",
    "    print(\"y_train\",np.unique(y_train))\n",
    "    # 2. TabNet 모델 학습\n",
    "    tabnet_clf = train_tabnet_classifier(X_train, y_train, X_valid, y_valid)\n",
    "    \n",
    "\n",
    "    # 3. 모델 저장\n",
    "    model_save_path = path+f'\\\\model_hist\\\\{generation}_tabnet_model.pickle'\n",
    "    save_model(tabnet_clf, model_save_path)\n",
    "    \n",
    "    # 4. 예측\n",
    "    predict = tabnet_clf.predict(X_test)\n",
    "    \n",
    "    \n",
    "    # 각 메트릭 계산\n",
    "    accuracy = accuracy_score(y_test, predict)\n",
    "    f1 = f1_score(y_test, predict, average='weighted')\n",
    "    precision, recall = sk(y_test, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-TabNet\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_TabNet_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT_model = DecisionTreeClassifier(\n",
    "     max_depth=6,\n",
    "    min_samples_leaf=20,  # 노드를 분할하기 위한 최소 샘플 수를 증가\n",
    "    random_state=42,\n",
    "    max_features=1    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: smote\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: adasyn\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: copulagan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: ctgan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: nbsynthetic\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    print(f\"\\n처리 중인 생성 모델: {generation}\")\n",
    "        \n",
    "    # 1. 데이터 전처리\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = preprocess_data(generation)\n",
    "    X_train = np.concatenate((X_train, X_valid), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_valid), axis=0)\n",
    " \n",
    "\n",
    "         # 스케일링 적용\n",
    "    # scaler=StandardScaler()\n",
    "    # X_train=scaler.fit_transform(X_train)\n",
    "    # X_test=scaler.transform(X_test)\n",
    " \n",
    "    DT_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3. 모델 저장\n",
    "    model_save_path = path+f'\\\\model_hist\\\\{generation}_DT_model.pickle'\n",
    "    \n",
    "    # with open(path+f'\\\\scaler_hist\\\\{generation}_DT_scaler.pickle', 'wb') as f:\n",
    "    #     pickle.dump(scaler, f)\n",
    "    \n",
    "    predict = DT_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    # 각 메트릭 계산\n",
    "    accuracy = accuracy_score(y_test, predict)\n",
    "    f1 = f1_score(y_test, predict, average='weighted')\n",
    "    precision, recall = sk(y_test, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-Decision Tree\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_DT_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-Decision Tree</td>\n",
       "      <td>0.67164</td>\n",
       "      <td>0.58551</td>\n",
       "      <td>0.66131</td>\n",
       "      <td>0.67164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-Decision Tree</td>\n",
       "      <td>0.68715</td>\n",
       "      <td>0.63012</td>\n",
       "      <td>0.68189</td>\n",
       "      <td>0.68715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-Decision Tree</td>\n",
       "      <td>0.76955</td>\n",
       "      <td>0.75069</td>\n",
       "      <td>0.77667</td>\n",
       "      <td>0.76955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-Decision Tree</td>\n",
       "      <td>0.74221</td>\n",
       "      <td>0.69421</td>\n",
       "      <td>0.72333</td>\n",
       "      <td>0.74221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-Decision Tree</td>\n",
       "      <td>0.89231</td>\n",
       "      <td>0.88783</td>\n",
       "      <td>0.89886</td>\n",
       "      <td>0.89231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-Decision Tree   0.67164  0.58551    0.66131  0.67164\n",
       "1       adasyn-Decision Tree   0.68715  0.63012    0.68189  0.68715\n",
       "2    copulagan-Decision Tree   0.76955  0.75069    0.77667  0.76955\n",
       "3        ctgan-Decision Tree   0.74221  0.69421    0.72333  0.74221\n",
       "4  nbsynthetic-Decision Tree   0.89231  0.88783    0.89886  0.89231"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_DT_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: smote\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: adasyn\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: copulagan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: ctgan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: nbsynthetic\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    print(f\"\\n처리 중인 생성 모델: {generation}\")\n",
    "        \n",
    "    # 1. 데이터 전처리\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = preprocess_data(generation)\n",
    "    X_train = np.concatenate((X_train, X_valid), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_valid), axis=0)\n",
    "    \n",
    "    \n",
    "       # 스케일링 적용=> 필요하지 않음\n",
    "    # scaler=StandardScaler()\n",
    "    # X_train=scaler.fit_transform(X_train)\n",
    "    # X_test=scaler.transform(X_test)\n",
    " \n",
    "    NB_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3. 모델 저장\n",
    "    model_save_path = path+f'\\\\model_hist\\\\{generation}_NB_model.pickle'\n",
    "    \n",
    "    # with open(path+f'\\\\scaler_hist\\\\{generation}_NB_scaler.pickle', 'wb') as f:\n",
    "    #     pickle.dump(scaler, f)\n",
    "    \n",
    "    predict = NB_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 각 메트릭 계산\n",
    "    accuracy = accuracy_score(y_test, predict)\n",
    "    f1 = f1_score(y_test, predict, average='weighted')\n",
    "    precision, recall = sk(y_test, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-Naive Bayes\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_Naive_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-Naive Bayes</td>\n",
       "      <td>0.65308</td>\n",
       "      <td>0.54851</td>\n",
       "      <td>0.63280</td>\n",
       "      <td>0.65308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-Naive Bayes</td>\n",
       "      <td>0.65925</td>\n",
       "      <td>0.56415</td>\n",
       "      <td>0.59865</td>\n",
       "      <td>0.65925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-Naive Bayes</td>\n",
       "      <td>0.72300</td>\n",
       "      <td>0.69232</td>\n",
       "      <td>0.71592</td>\n",
       "      <td>0.72300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-Naive Bayes</td>\n",
       "      <td>0.67362</td>\n",
       "      <td>0.61800</td>\n",
       "      <td>0.64170</td>\n",
       "      <td>0.67362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-Naive Bayes</td>\n",
       "      <td>0.84427</td>\n",
       "      <td>0.83873</td>\n",
       "      <td>0.83724</td>\n",
       "      <td>0.84427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-Naive Bayes   0.65308  0.54851    0.63280  0.65308\n",
       "1       adasyn-Naive Bayes   0.65925  0.56415    0.59865  0.65925\n",
       "2    copulagan-Naive Bayes   0.72300  0.69232    0.71592  0.72300\n",
       "3        ctgan-Naive Bayes   0.67362  0.61800    0.64170  0.67362\n",
       "4  nbsynthetic-Naive Bayes   0.84427  0.83873    0.83724  0.84427"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Naive_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDA_model = LinearDiscriminantAnalysis(\n",
    "    solver='lsqr', \n",
    "    shrinkage=0.9,           \n",
    "    n_components=2,           \n",
    "    tol=1e-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: smote\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: adasyn\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: copulagan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: ctgan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: nbsynthetic\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    print(f\"\\n처리 중인 생성 모델: {generation}\")\n",
    "        \n",
    "    # 1. 데이터 전처리\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = preprocess_data(generation)\n",
    "    X_train = np.concatenate((X_train, X_valid), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_valid), axis=0)\n",
    "\n",
    "    \n",
    "    # 스케일링 적용\n",
    "    scaler=StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_test=scaler.transform(X_test)\n",
    " \n",
    "    LDA_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3. 모델 저장\n",
    "    model_save_path = path+f'\\\\model_hist\\\\{generation}_LDA_model.pickle'\n",
    "    \n",
    "    with open(path+f'\\\\scaler_hist\\\\{generation}_LDA_scaler.pickle', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    predict = LDA_model.predict(X_test)\n",
    "    \n",
    "    # 각 메트릭 계산\n",
    "    accuracy = accuracy_score(y_test, predict)\n",
    "    f1 = f1_score(y_test, predict, average='weighted')\n",
    "    precision, recall = sk(y_test, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-LDA\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_LDA_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-LDA</td>\n",
       "      <td>0.64761</td>\n",
       "      <td>0.52650</td>\n",
       "      <td>0.49422</td>\n",
       "      <td>0.64761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-LDA</td>\n",
       "      <td>0.64796</td>\n",
       "      <td>0.52353</td>\n",
       "      <td>0.55690</td>\n",
       "      <td>0.64796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-LDA</td>\n",
       "      <td>0.72979</td>\n",
       "      <td>0.70100</td>\n",
       "      <td>0.72794</td>\n",
       "      <td>0.72979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-LDA</td>\n",
       "      <td>0.67396</td>\n",
       "      <td>0.61543</td>\n",
       "      <td>0.64449</td>\n",
       "      <td>0.67396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-LDA</td>\n",
       "      <td>0.82207</td>\n",
       "      <td>0.81463</td>\n",
       "      <td>0.81122</td>\n",
       "      <td>0.82207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-LDA   0.64761  0.52650    0.49422  0.64761\n",
       "1       adasyn-LDA   0.64796  0.52353    0.55690  0.64796\n",
       "2    copulagan-LDA   0.72979  0.70100    0.72794  0.72979\n",
       "3        ctgan-LDA   0.67396  0.61543    0.64449  0.67396\n",
       "4  nbsynthetic-LDA   0.82207  0.81463    0.81122  0.82207"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_LDA_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: smote\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: adasyn\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: copulagan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: ctgan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "did not converge\n",
      "\n",
      "처리 중인 생성 모델: nbsynthetic\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support as sk\n",
    "from pygam import LogisticGAM, s\n",
    "\n",
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    print(f\"\\n처리 중인 생성 모델: {generation}\")        \n",
    "    # 1. 데이터 전처리\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = preprocess_data(generation)\n",
    "    \n",
    "    X_train = np.concatenate((X_train, X_valid), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_valid), axis=0)\n",
    "    \n",
    "    # 레이블 인코딩\n",
    "    le = LabelEncoder()\n",
    "    y_train_encoded = le.fit_transform(y_train)\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "    n_classes = len(np.unique(y_train_encoded))\n",
    "    \n",
    "    \n",
    "      # 스케일링 적용\n",
    "    scaler=StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_test=scaler.transform(X_test)\n",
    "\n",
    "    with open(path+f'\\\\scaler_hist\\\\{generation}_gams_scaler.pickle', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "        \n",
    "    # 각 클래스별 GAM 모델 학습 (One-vs-Rest 방식)\n",
    "    gam_models = []\n",
    "    for class_idx in range(n_classes):\n",
    "        y_binary = (y_train_encoded == class_idx).astype(int)\n",
    "        \n",
    "        formula = s(0) + s(1) + s(2)  \n",
    "        gam = LogisticGAM(formula)\n",
    "        gam.fit(X_train, y_binary)\n",
    "        gam_models.append(gam)\n",
    "    \n",
    "    # 3. 모델 저장\n",
    "    model_save_path = path+f'\\\\model_hist\\\\{generation}_gams_model.pickle'\n",
    "    with open(model_save_path, 'wb') as f:\n",
    "        pickle.dump(gam_models, f)\n",
    "    \n",
    "    # 각 클래스에 대한 예측 확률 계산\n",
    "    probs = np.column_stack([\n",
    "        gam.predict_proba(X_test) for gam in gam_models\n",
    "    ])\n",
    "    \n",
    "    # 가장 높은 확률을 가진 클래스 선택\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "    \n",
    "    # 각 메트릭 계산\n",
    "    accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "    f1 = f1_score(y_test_encoded, y_pred, average='weighted')\n",
    "    precision, recall = sk(y_test_encoded, y_pred, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-GAMs\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_GAMs_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_GAMs_results.to_csv(path+'\\\\GAMs_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54862, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임을 CSV 파일로 저장\n",
    "# df_GAMs_results.to_csv(path + f'\\\\results\\\\GAMs_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-GAMs</td>\n",
       "      <td>0.65209</td>\n",
       "      <td>0.55527</td>\n",
       "      <td>0.59903</td>\n",
       "      <td>0.65209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-GAMs</td>\n",
       "      <td>0.66456</td>\n",
       "      <td>0.56873</td>\n",
       "      <td>0.61389</td>\n",
       "      <td>0.66456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-GAMs</td>\n",
       "      <td>0.77419</td>\n",
       "      <td>0.75511</td>\n",
       "      <td>0.77120</td>\n",
       "      <td>0.77419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-GAMs</td>\n",
       "      <td>0.73741</td>\n",
       "      <td>0.69963</td>\n",
       "      <td>0.74725</td>\n",
       "      <td>0.73741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-GAMs</td>\n",
       "      <td>0.79639</td>\n",
       "      <td>0.78008</td>\n",
       "      <td>0.79083</td>\n",
       "      <td>0.79639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-GAMs   0.65209  0.55527    0.59903  0.65209\n",
       "1       adasyn-GAMs   0.66456  0.56873    0.61389  0.66456\n",
       "2    copulagan-GAMs   0.77419  0.75511    0.77120  0.77419\n",
       "3        ctgan-GAMs   0.73741  0.69963    0.74725  0.73741\n",
       "4  nbsynthetic-GAMs   0.79639  0.78008    0.79083  0.79639"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GAMs_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 총합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-Naive Bayes</td>\n",
       "      <td>0.65308</td>\n",
       "      <td>0.54851</td>\n",
       "      <td>0.63280</td>\n",
       "      <td>0.65308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-Naive Bayes</td>\n",
       "      <td>0.65925</td>\n",
       "      <td>0.56415</td>\n",
       "      <td>0.59865</td>\n",
       "      <td>0.65925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-Naive Bayes</td>\n",
       "      <td>0.72300</td>\n",
       "      <td>0.69232</td>\n",
       "      <td>0.71592</td>\n",
       "      <td>0.72300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-Naive Bayes</td>\n",
       "      <td>0.67362</td>\n",
       "      <td>0.61800</td>\n",
       "      <td>0.64170</td>\n",
       "      <td>0.67362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-Naive Bayes</td>\n",
       "      <td>0.84427</td>\n",
       "      <td>0.83873</td>\n",
       "      <td>0.83724</td>\n",
       "      <td>0.84427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-Naive Bayes   0.65308  0.54851    0.63280  0.65308\n",
       "1       adasyn-Naive Bayes   0.65925  0.56415    0.59865  0.65925\n",
       "2    copulagan-Naive Bayes   0.72300  0.69232    0.71592  0.72300\n",
       "3        ctgan-Naive Bayes   0.67362  0.61800    0.64170  0.67362\n",
       "4  nbsynthetic-Naive Bayes   0.84427  0.83873    0.83724  0.84427"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Naive_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-LDA</td>\n",
       "      <td>0.64761</td>\n",
       "      <td>0.52650</td>\n",
       "      <td>0.49422</td>\n",
       "      <td>0.64761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-LDA</td>\n",
       "      <td>0.64796</td>\n",
       "      <td>0.52353</td>\n",
       "      <td>0.55690</td>\n",
       "      <td>0.64796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-LDA</td>\n",
       "      <td>0.72979</td>\n",
       "      <td>0.70100</td>\n",
       "      <td>0.72794</td>\n",
       "      <td>0.72979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-LDA</td>\n",
       "      <td>0.67396</td>\n",
       "      <td>0.61543</td>\n",
       "      <td>0.64449</td>\n",
       "      <td>0.67396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-LDA</td>\n",
       "      <td>0.82207</td>\n",
       "      <td>0.81463</td>\n",
       "      <td>0.81122</td>\n",
       "      <td>0.82207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-LDA   0.64761  0.52650    0.49422  0.64761\n",
       "1       adasyn-LDA   0.64796  0.52353    0.55690  0.64796\n",
       "2    copulagan-LDA   0.72979  0.70100    0.72794  0.72979\n",
       "3        ctgan-LDA   0.67396  0.61543    0.64449  0.67396\n",
       "4  nbsynthetic-LDA   0.82207  0.81463    0.81122  0.82207"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_LDA_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml=pd.concat([df_TabNet_results,df_DT_results,df_Naive_results,df_LDA_results,df_GAMs_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml=df_ml.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.to_csv(path+'\\\\data\\\\머신 러닝 통합 결과정리.csv',encoding='cp949',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-TabNet</td>\n",
       "      <td>0.87989</td>\n",
       "      <td>0.87995</td>\n",
       "      <td>0.88011</td>\n",
       "      <td>0.87989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-TabNet</td>\n",
       "      <td>0.86732</td>\n",
       "      <td>0.86492</td>\n",
       "      <td>0.86395</td>\n",
       "      <td>0.86732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-TabNet</td>\n",
       "      <td>0.87840</td>\n",
       "      <td>0.87673</td>\n",
       "      <td>0.87623</td>\n",
       "      <td>0.87840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-TabNet</td>\n",
       "      <td>0.87028</td>\n",
       "      <td>0.86519</td>\n",
       "      <td>0.86627</td>\n",
       "      <td>0.87028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-TabNet</td>\n",
       "      <td>0.96918</td>\n",
       "      <td>0.96878</td>\n",
       "      <td>0.96992</td>\n",
       "      <td>0.96918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>smote-Decision Tree</td>\n",
       "      <td>0.67164</td>\n",
       "      <td>0.58551</td>\n",
       "      <td>0.66131</td>\n",
       "      <td>0.67164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adasyn-Decision Tree</td>\n",
       "      <td>0.68715</td>\n",
       "      <td>0.63012</td>\n",
       "      <td>0.68189</td>\n",
       "      <td>0.68715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>copulagan-Decision Tree</td>\n",
       "      <td>0.76955</td>\n",
       "      <td>0.75069</td>\n",
       "      <td>0.77667</td>\n",
       "      <td>0.76955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ctgan-Decision Tree</td>\n",
       "      <td>0.74221</td>\n",
       "      <td>0.69421</td>\n",
       "      <td>0.72333</td>\n",
       "      <td>0.74221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nbsynthetic-Decision Tree</td>\n",
       "      <td>0.89231</td>\n",
       "      <td>0.88783</td>\n",
       "      <td>0.89886</td>\n",
       "      <td>0.89231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>smote-Naive Bayes</td>\n",
       "      <td>0.65308</td>\n",
       "      <td>0.54851</td>\n",
       "      <td>0.63280</td>\n",
       "      <td>0.65308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>adasyn-Naive Bayes</td>\n",
       "      <td>0.65925</td>\n",
       "      <td>0.56415</td>\n",
       "      <td>0.59865</td>\n",
       "      <td>0.65925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>copulagan-Naive Bayes</td>\n",
       "      <td>0.72300</td>\n",
       "      <td>0.69232</td>\n",
       "      <td>0.71592</td>\n",
       "      <td>0.72300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ctgan-Naive Bayes</td>\n",
       "      <td>0.67362</td>\n",
       "      <td>0.61800</td>\n",
       "      <td>0.64170</td>\n",
       "      <td>0.67362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nbsynthetic-Naive Bayes</td>\n",
       "      <td>0.84427</td>\n",
       "      <td>0.83873</td>\n",
       "      <td>0.83724</td>\n",
       "      <td>0.84427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>smote-LDA</td>\n",
       "      <td>0.64761</td>\n",
       "      <td>0.52650</td>\n",
       "      <td>0.49422</td>\n",
       "      <td>0.64761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adasyn-LDA</td>\n",
       "      <td>0.64796</td>\n",
       "      <td>0.52353</td>\n",
       "      <td>0.55690</td>\n",
       "      <td>0.64796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>copulagan-LDA</td>\n",
       "      <td>0.72979</td>\n",
       "      <td>0.70100</td>\n",
       "      <td>0.72794</td>\n",
       "      <td>0.72979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ctgan-LDA</td>\n",
       "      <td>0.67396</td>\n",
       "      <td>0.61543</td>\n",
       "      <td>0.64449</td>\n",
       "      <td>0.67396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nbsynthetic-LDA</td>\n",
       "      <td>0.82207</td>\n",
       "      <td>0.81463</td>\n",
       "      <td>0.81122</td>\n",
       "      <td>0.82207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>smote-GAMs</td>\n",
       "      <td>0.65209</td>\n",
       "      <td>0.55527</td>\n",
       "      <td>0.59903</td>\n",
       "      <td>0.65209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>adasyn-GAMs</td>\n",
       "      <td>0.66456</td>\n",
       "      <td>0.56873</td>\n",
       "      <td>0.61389</td>\n",
       "      <td>0.66456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>copulagan-GAMs</td>\n",
       "      <td>0.77419</td>\n",
       "      <td>0.75511</td>\n",
       "      <td>0.77120</td>\n",
       "      <td>0.77419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ctgan-GAMs</td>\n",
       "      <td>0.73741</td>\n",
       "      <td>0.69963</td>\n",
       "      <td>0.74725</td>\n",
       "      <td>0.73741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>nbsynthetic-GAMs</td>\n",
       "      <td>0.79639</td>\n",
       "      <td>0.78008</td>\n",
       "      <td>0.79083</td>\n",
       "      <td>0.79639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy  F1score  Precision   Recall\n",
       "0                smote-TabNet   0.87989  0.87995    0.88011  0.87989\n",
       "1               adasyn-TabNet   0.86732  0.86492    0.86395  0.86732\n",
       "2            copulagan-TabNet   0.87840  0.87673    0.87623  0.87840\n",
       "3                ctgan-TabNet   0.87028  0.86519    0.86627  0.87028\n",
       "4          nbsynthetic-TabNet   0.96918  0.96878    0.96992  0.96918\n",
       "5         smote-Decision Tree   0.67164  0.58551    0.66131  0.67164\n",
       "6        adasyn-Decision Tree   0.68715  0.63012    0.68189  0.68715\n",
       "7     copulagan-Decision Tree   0.76955  0.75069    0.77667  0.76955\n",
       "8         ctgan-Decision Tree   0.74221  0.69421    0.72333  0.74221\n",
       "9   nbsynthetic-Decision Tree   0.89231  0.88783    0.89886  0.89231\n",
       "10          smote-Naive Bayes   0.65308  0.54851    0.63280  0.65308\n",
       "11         adasyn-Naive Bayes   0.65925  0.56415    0.59865  0.65925\n",
       "12      copulagan-Naive Bayes   0.72300  0.69232    0.71592  0.72300\n",
       "13          ctgan-Naive Bayes   0.67362  0.61800    0.64170  0.67362\n",
       "14    nbsynthetic-Naive Bayes   0.84427  0.83873    0.83724  0.84427\n",
       "15                  smote-LDA   0.64761  0.52650    0.49422  0.64761\n",
       "16                 adasyn-LDA   0.64796  0.52353    0.55690  0.64796\n",
       "17              copulagan-LDA   0.72979  0.70100    0.72794  0.72979\n",
       "18                  ctgan-LDA   0.67396  0.61543    0.64449  0.67396\n",
       "19            nbsynthetic-LDA   0.82207  0.81463    0.81122  0.82207\n",
       "20                 smote-GAMs   0.65209  0.55527    0.59903  0.65209\n",
       "21                adasyn-GAMs   0.66456  0.56873    0.61389  0.66456\n",
       "22             copulagan-GAMs   0.77419  0.75511    0.77120  0.77419\n",
       "23                 ctgan-GAMs   0.73741  0.69963    0.74725  0.73741\n",
       "24           nbsynthetic-GAMs   0.79639  0.78008    0.79083  0.79639"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jm_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
