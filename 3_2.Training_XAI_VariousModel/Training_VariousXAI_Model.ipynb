{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 설명가능한 모델별로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 방법으로 합성한 데이터와 여러개의 XAI 모델을 조합하여 경우의 수 모델학습   \n",
    "합성데이터(5가지): Nbsynthetic(Wgan), CopulaGAN, CTGAN, Smote, ADASYN  \n",
    "모델: TabNet, Naive Bayes, Decision Tree, GAMs, LDA  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델별 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제안한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support as sk\n",
    "from sklearn.metrics import f1_score ## F1 Score 구하기\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "# 로컬 모듈 임포트\n",
    "from data_preparation import preprocess_data, convert_to_tensor\n",
    "from encoding_tabnet import train_tabnet_classifier, extract_tabnet_embeddings\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score\n",
    "from utils import standardize_features, calculate_metrics, save_model, print_results\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 프로젝트 루트 디렉토리 경로 (현재 파일의 상위 디렉토리)\n",
    "path = os.path.abspath('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\jupyter\\\\Explainable Healthcare framework\\\\Explainable_healthcareframework\\\\3_2.Training_XAI_VariousModel'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generation_list = ['smote','adasyn','copulagan','ctgan','nbsynthetic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: smote\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "y_train [0 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.05153 | val_0_accuracy: 0.63132 |  0:00:08s\n",
      "epoch 1  | loss: 0.78078 | val_0_accuracy: 0.40858 |  0:00:16s\n",
      "epoch 2  | loss: 0.74854 | val_0_accuracy: 0.20295 |  0:00:24s\n",
      "epoch 3  | loss: 0.71564 | val_0_accuracy: 0.6219  |  0:00:32s\n",
      "epoch 4  | loss: 0.67268 | val_0_accuracy: 0.63242 |  0:00:41s\n",
      "epoch 5  | loss: 0.63517 | val_0_accuracy: 0.63839 |  0:00:49s\n",
      "epoch 6  | loss: 0.62186 | val_0_accuracy: 0.59488 |  0:00:57s\n",
      "epoch 7  | loss: 0.63708 | val_0_accuracy: 0.51649 |  0:01:05s\n",
      "epoch 8  | loss: 0.63777 | val_0_accuracy: 0.50754 |  0:01:13s\n",
      "epoch 9  | loss: 0.60528 | val_0_accuracy: 0.60823 |  0:01:21s\n",
      "epoch 10 | loss: 0.58379 | val_0_accuracy: 0.62268 |  0:01:29s\n",
      "epoch 11 | loss: 0.56487 | val_0_accuracy: 0.62237 |  0:01:37s\n",
      "epoch 12 | loss: 0.55908 | val_0_accuracy: 0.62158 |  0:01:44s\n",
      "epoch 13 | loss: 0.55136 | val_0_accuracy: 0.63981 |  0:01:52s\n",
      "epoch 14 | loss: 0.5202  | val_0_accuracy: 0.63713 |  0:02:01s\n",
      "epoch 15 | loss: 0.53246 | val_0_accuracy: 0.63085 |  0:02:09s\n",
      "epoch 16 | loss: 0.52293 | val_0_accuracy: 0.61844 |  0:02:17s\n",
      "epoch 17 | loss: 0.51929 | val_0_accuracy: 0.63007 |  0:02:25s\n",
      "epoch 18 | loss: 0.49747 | val_0_accuracy: 0.61986 |  0:02:32s\n",
      "epoch 19 | loss: 0.49283 | val_0_accuracy: 0.63776 |  0:02:40s\n",
      "epoch 20 | loss: 0.49963 | val_0_accuracy: 0.65112 |  0:02:49s\n",
      "epoch 21 | loss: 0.50138 | val_0_accuracy: 0.64075 |  0:02:57s\n",
      "epoch 22 | loss: 0.49904 | val_0_accuracy: 0.62096 |  0:03:04s\n",
      "epoch 23 | loss: 0.47714 | val_0_accuracy: 0.64577 |  0:03:12s\n",
      "epoch 24 | loss: 0.46762 | val_0_accuracy: 0.64248 |  0:03:20s\n",
      "epoch 25 | loss: 0.47302 | val_0_accuracy: 0.63022 |  0:03:28s\n",
      "epoch 26 | loss: 0.47568 | val_0_accuracy: 0.65489 |  0:03:36s\n",
      "epoch 27 | loss: 0.46939 | val_0_accuracy: 0.64452 |  0:03:44s\n",
      "epoch 28 | loss: 0.45297 | val_0_accuracy: 0.67185 |  0:03:52s\n",
      "epoch 29 | loss: 0.46615 | val_0_accuracy: 0.66101 |  0:03:59s\n",
      "epoch 30 | loss: 0.45464 | val_0_accuracy: 0.65112 |  0:04:07s\n",
      "epoch 31 | loss: 0.42231 | val_0_accuracy: 0.66871 |  0:04:15s\n",
      "epoch 32 | loss: 0.41009 | val_0_accuracy: 0.67923 |  0:04:22s\n",
      "epoch 33 | loss: 0.40762 | val_0_accuracy: 0.68567 |  0:04:30s\n",
      "epoch 34 | loss: 0.4377  | val_0_accuracy: 0.68662 |  0:04:38s\n",
      "epoch 35 | loss: 0.44093 | val_0_accuracy: 0.7017  |  0:04:46s\n",
      "epoch 36 | loss: 0.41937 | val_0_accuracy: 0.70217 |  0:04:54s\n",
      "epoch 37 | loss: 0.41751 | val_0_accuracy: 0.72227 |  0:05:02s\n",
      "epoch 38 | loss: 0.42966 | val_0_accuracy: 0.71285 |  0:05:10s\n",
      "epoch 39 | loss: 0.41214 | val_0_accuracy: 0.71112 |  0:05:18s\n",
      "epoch 40 | loss: 0.39985 | val_0_accuracy: 0.74757 |  0:05:26s\n",
      "epoch 41 | loss: 0.3785  | val_0_accuracy: 0.75511 |  0:05:34s\n",
      "epoch 42 | loss: 0.38342 | val_0_accuracy: 0.75699 |  0:05:42s\n",
      "epoch 43 | loss: 0.42032 | val_0_accuracy: 0.74961 |  0:05:51s\n",
      "epoch 44 | loss: 0.39379 | val_0_accuracy: 0.76437 |  0:05:58s\n",
      "epoch 45 | loss: 0.37942 | val_0_accuracy: 0.77553 |  0:06:06s\n",
      "epoch 46 | loss: 0.37156 | val_0_accuracy: 0.78935 |  0:06:14s\n",
      "epoch 47 | loss: 0.38544 | val_0_accuracy: 0.78841 |  0:06:22s\n",
      "epoch 48 | loss: 0.40231 | val_0_accuracy: 0.78778 |  0:06:30s\n",
      "epoch 49 | loss: 0.39296 | val_0_accuracy: 0.79218 |  0:06:38s\n",
      "epoch 50 | loss: 0.37436 | val_0_accuracy: 0.78024 |  0:06:46s\n",
      "epoch 51 | loss: 0.36412 | val_0_accuracy: 0.81872 |  0:06:54s\n",
      "epoch 52 | loss: 0.35956 | val_0_accuracy: 0.81762 |  0:07:03s\n",
      "epoch 53 | loss: 0.38618 | val_0_accuracy: 0.79972 |  0:07:11s\n",
      "epoch 54 | loss: 0.36899 | val_0_accuracy: 0.81448 |  0:07:19s\n",
      "epoch 55 | loss: 0.34987 | val_0_accuracy: 0.83742 |  0:07:28s\n",
      "epoch 56 | loss: 0.3468  | val_0_accuracy: 0.82171 |  0:07:36s\n",
      "epoch 57 | loss: 0.3392  | val_0_accuracy: 0.8338  |  0:07:45s\n",
      "epoch 58 | loss: 0.34842 | val_0_accuracy: 0.85894 |  0:07:54s\n",
      "epoch 59 | loss: 0.32468 | val_0_accuracy: 0.86349 |  0:08:02s\n",
      "epoch 60 | loss: 0.32039 | val_0_accuracy: 0.86129 |  0:08:11s\n",
      "epoch 61 | loss: 0.31376 | val_0_accuracy: 0.85611 |  0:08:19s\n",
      "epoch 62 | loss: 0.31775 | val_0_accuracy: 0.85815 |  0:08:27s\n",
      "epoch 63 | loss: 0.30556 | val_0_accuracy: 0.86899 |  0:08:36s\n",
      "epoch 64 | loss: 0.31378 | val_0_accuracy: 0.8492  |  0:08:44s\n",
      "epoch 65 | loss: 0.3124  | val_0_accuracy: 0.85627 |  0:08:52s\n",
      "epoch 66 | loss: 0.30858 | val_0_accuracy: 0.86695 |  0:09:01s\n",
      "epoch 67 | loss: 0.31412 | val_0_accuracy: 0.85894 |  0:09:09s\n",
      "epoch 68 | loss: 0.30938 | val_0_accuracy: 0.85674 |  0:09:17s\n",
      "epoch 69 | loss: 0.30158 | val_0_accuracy: 0.88501 |  0:09:25s\n",
      "epoch 70 | loss: 0.30424 | val_0_accuracy: 0.87857 |  0:09:33s\n",
      "epoch 71 | loss: 0.31182 | val_0_accuracy: 0.85328 |  0:09:41s\n",
      "epoch 72 | loss: 0.30949 | val_0_accuracy: 0.85752 |  0:09:49s\n",
      "epoch 73 | loss: 0.29882 | val_0_accuracy: 0.88281 |  0:09:57s\n",
      "epoch 74 | loss: 0.29766 | val_0_accuracy: 0.86082 |  0:10:05s\n",
      "epoch 75 | loss: 0.32792 | val_0_accuracy: 0.84025 |  0:10:13s\n",
      "epoch 76 | loss: 0.32395 | val_0_accuracy: 0.86224 |  0:10:21s\n",
      "epoch 77 | loss: 0.31635 | val_0_accuracy: 0.86129 |  0:10:29s\n",
      "epoch 78 | loss: 0.29393 | val_0_accuracy: 0.86004 |  0:10:37s\n",
      "epoch 79 | loss: 0.2924  | val_0_accuracy: 0.87009 |  0:10:45s\n",
      "epoch 80 | loss: 0.28581 | val_0_accuracy: 0.8781  |  0:10:54s\n",
      "epoch 81 | loss: 0.29297 | val_0_accuracy: 0.87496 |  0:11:01s\n",
      "epoch 82 | loss: 0.30723 | val_0_accuracy: 0.87669 |  0:11:09s\n",
      "epoch 83 | loss: 0.30463 | val_0_accuracy: 0.85501 |  0:11:17s\n",
      "epoch 84 | loss: 0.29325 | val_0_accuracy: 0.86962 |  0:11:25s\n",
      "epoch 85 | loss: 0.28513 | val_0_accuracy: 0.86773 |  0:11:33s\n",
      "epoch 86 | loss: 0.28251 | val_0_accuracy: 0.87512 |  0:11:41s\n",
      "epoch 87 | loss: 0.29012 | val_0_accuracy: 0.86161 |  0:11:49s\n",
      "epoch 88 | loss: 0.29477 | val_0_accuracy: 0.85595 |  0:11:57s\n",
      "epoch 89 | loss: 0.30602 | val_0_accuracy: 0.86601 |  0:12:05s\n",
      "epoch 90 | loss: 0.29618 | val_0_accuracy: 0.84826 |  0:12:13s\n",
      "epoch 91 | loss: 0.28754 | val_0_accuracy: 0.85752 |  0:12:23s\n",
      "epoch 92 | loss: 0.30082 | val_0_accuracy: 0.86145 |  0:12:34s\n",
      "epoch 93 | loss: 0.28623 | val_0_accuracy: 0.86616 |  0:12:42s\n",
      "epoch 94 | loss: 0.2711  | val_0_accuracy: 0.86962 |  0:12:50s\n",
      "epoch 95 | loss: 0.26529 | val_0_accuracy: 0.87465 |  0:12:58s\n",
      "epoch 96 | loss: 0.26218 | val_0_accuracy: 0.86664 |  0:13:07s\n",
      "epoch 97 | loss: 0.2719  | val_0_accuracy: 0.86758 |  0:13:15s\n",
      "epoch 98 | loss: 0.27448 | val_0_accuracy: 0.88329 |  0:13:23s\n",
      "epoch 99 | loss: 0.27865 | val_0_accuracy: 0.86098 |  0:13:31s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 69 and best_val_0_accuracy = 0.88501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: adasyn\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "y_train [0 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.06989 | val_0_accuracy: 0.2224  |  0:00:07s\n",
      "epoch 1  | loss: 0.78991 | val_0_accuracy: 0.56432 |  0:00:16s\n",
      "epoch 2  | loss: 0.74178 | val_0_accuracy: 0.23575 |  0:00:24s\n",
      "epoch 3  | loss: 0.69955 | val_0_accuracy: 0.55458 |  0:00:32s\n",
      "epoch 4  | loss: 0.6875  | val_0_accuracy: 0.29072 |  0:00:40s\n",
      "epoch 5  | loss: 0.67144 | val_0_accuracy: 0.33234 |  0:00:48s\n",
      "epoch 6  | loss: 0.67846 | val_0_accuracy: 0.28679 |  0:00:56s\n",
      "epoch 7  | loss: 0.65056 | val_0_accuracy: 0.30203 |  0:01:04s\n",
      "epoch 8  | loss: 0.63406 | val_0_accuracy: 0.29464 |  0:01:12s\n",
      "epoch 9  | loss: 0.60617 | val_0_accuracy: 0.22208 |  0:01:20s\n",
      "epoch 10 | loss: 0.58487 | val_0_accuracy: 0.22255 |  0:01:28s\n",
      "epoch 11 | loss: 0.60594 | val_0_accuracy: 0.24423 |  0:01:36s\n",
      "epoch 12 | loss: 0.6198  | val_0_accuracy: 0.30124 |  0:01:44s\n",
      "epoch 13 | loss: 0.59409 | val_0_accuracy: 0.2359  |  0:01:52s\n",
      "epoch 14 | loss: 0.56674 | val_0_accuracy: 0.2758  |  0:02:00s\n",
      "epoch 15 | loss: 0.54348 | val_0_accuracy: 0.27815 |  0:02:08s\n",
      "epoch 16 | loss: 0.55128 | val_0_accuracy: 0.37145 |  0:02:16s\n",
      "epoch 17 | loss: 0.5588  | val_0_accuracy: 0.44778 |  0:02:25s\n",
      "epoch 18 | loss: 0.55006 | val_0_accuracy: 0.45249 |  0:02:33s\n",
      "epoch 19 | loss: 0.51804 | val_0_accuracy: 0.48437 |  0:02:41s\n",
      "epoch 20 | loss: 0.50681 | val_0_accuracy: 0.55034 |  0:02:49s\n",
      "epoch 21 | loss: 0.49361 | val_0_accuracy: 0.57672 |  0:02:57s\n",
      "epoch 22 | loss: 0.51377 | val_0_accuracy: 0.64662 |  0:03:05s\n",
      "epoch 23 | loss: 0.50415 | val_0_accuracy: 0.62494 |  0:03:13s\n",
      "epoch 24 | loss: 0.52368 | val_0_accuracy: 0.61442 |  0:03:21s\n",
      "epoch 25 | loss: 0.52051 | val_0_accuracy: 0.61112 |  0:03:30s\n",
      "epoch 26 | loss: 0.49512 | val_0_accuracy: 0.64002 |  0:03:38s\n",
      "epoch 27 | loss: 0.49133 | val_0_accuracy: 0.66421 |  0:03:47s\n",
      "epoch 28 | loss: 0.47127 | val_0_accuracy: 0.65965 |  0:03:55s\n",
      "epoch 29 | loss: 0.47387 | val_0_accuracy: 0.66483 |  0:04:03s\n",
      "epoch 30 | loss: 0.45977 | val_0_accuracy: 0.65557 |  0:04:11s\n",
      "epoch 31 | loss: 0.45692 | val_0_accuracy: 0.68085 |  0:04:19s\n",
      "epoch 32 | loss: 0.45956 | val_0_accuracy: 0.69138 |  0:04:27s\n",
      "epoch 33 | loss: 0.45363 | val_0_accuracy: 0.69515 |  0:04:35s\n",
      "epoch 34 | loss: 0.45239 | val_0_accuracy: 0.69656 |  0:04:43s\n",
      "epoch 35 | loss: 0.44383 | val_0_accuracy: 0.70536 |  0:04:51s\n",
      "epoch 36 | loss: 0.4418  | val_0_accuracy: 0.70755 |  0:04:59s\n",
      "epoch 37 | loss: 0.43482 | val_0_accuracy: 0.72122 |  0:05:07s\n",
      "epoch 38 | loss: 0.43704 | val_0_accuracy: 0.72813 |  0:05:15s\n",
      "epoch 39 | loss: 0.41918 | val_0_accuracy: 0.72075 |  0:05:23s\n",
      "epoch 40 | loss: 0.4031  | val_0_accuracy: 0.74148 |  0:05:31s\n",
      "epoch 41 | loss: 0.40163 | val_0_accuracy: 0.73865 |  0:05:38s\n",
      "epoch 42 | loss: 0.39645 | val_0_accuracy: 0.77289 |  0:05:46s\n",
      "epoch 43 | loss: 0.40484 | val_0_accuracy: 0.75389 |  0:05:54s\n",
      "epoch 44 | loss: 0.40623 | val_0_accuracy: 0.77054 |  0:06:02s\n",
      "epoch 45 | loss: 0.39884 | val_0_accuracy: 0.77525 |  0:06:10s\n",
      "epoch 46 | loss: 0.39321 | val_0_accuracy: 0.79771 |  0:06:17s\n",
      "epoch 47 | loss: 0.40514 | val_0_accuracy: 0.76928 |  0:06:25s\n",
      "epoch 48 | loss: 0.39512 | val_0_accuracy: 0.77493 |  0:06:33s\n",
      "epoch 49 | loss: 0.3991  | val_0_accuracy: 0.7831  |  0:06:41s\n",
      "epoch 50 | loss: 0.39241 | val_0_accuracy: 0.78608 |  0:06:49s\n",
      "epoch 51 | loss: 0.38164 | val_0_accuracy: 0.78122 |  0:06:56s\n",
      "epoch 52 | loss: 0.36742 | val_0_accuracy: 0.79943 |  0:07:04s\n",
      "epoch 53 | loss: 0.35972 | val_0_accuracy: 0.79111 |  0:07:12s\n",
      "epoch 54 | loss: 0.34813 | val_0_accuracy: 0.79834 |  0:07:20s\n",
      "epoch 55 | loss: 0.35608 | val_0_accuracy: 0.81907 |  0:07:28s\n",
      "epoch 56 | loss: 0.35349 | val_0_accuracy: 0.81812 |  0:07:35s\n",
      "epoch 57 | loss: 0.36133 | val_0_accuracy: 0.8164  |  0:07:43s\n",
      "epoch 58 | loss: 0.36275 | val_0_accuracy: 0.81137 |  0:07:51s\n",
      "epoch 59 | loss: 0.37177 | val_0_accuracy: 0.7897  |  0:07:59s\n",
      "epoch 60 | loss: 0.39283 | val_0_accuracy: 0.79048 |  0:08:07s\n",
      "epoch 61 | loss: 0.38021 | val_0_accuracy: 0.81373 |  0:08:15s\n",
      "epoch 62 | loss: 0.3656  | val_0_accuracy: 0.80226 |  0:08:22s\n",
      "epoch 63 | loss: 0.35801 | val_0_accuracy: 0.82048 |  0:08:30s\n",
      "epoch 64 | loss: 0.35027 | val_0_accuracy: 0.8131  |  0:08:38s\n",
      "epoch 65 | loss: 0.34432 | val_0_accuracy: 0.83477 |  0:08:46s\n",
      "epoch 66 | loss: 0.33909 | val_0_accuracy: 0.84828 |  0:08:54s\n",
      "epoch 67 | loss: 0.33149 | val_0_accuracy: 0.83242 |  0:09:01s\n",
      "epoch 68 | loss: 0.33607 | val_0_accuracy: 0.83823 |  0:09:09s\n",
      "epoch 69 | loss: 0.34571 | val_0_accuracy: 0.83257 |  0:09:17s\n",
      "epoch 70 | loss: 0.33262 | val_0_accuracy: 0.83666 |  0:09:25s\n",
      "epoch 71 | loss: 0.33415 | val_0_accuracy: 0.82629 |  0:09:33s\n",
      "epoch 72 | loss: 0.33435 | val_0_accuracy: 0.81624 |  0:09:41s\n",
      "epoch 73 | loss: 0.32878 | val_0_accuracy: 0.86053 |  0:09:49s\n",
      "epoch 74 | loss: 0.32799 | val_0_accuracy: 0.82755 |  0:09:57s\n",
      "epoch 75 | loss: 0.33138 | val_0_accuracy: 0.78561 |  0:10:04s\n",
      "epoch 76 | loss: 0.31792 | val_0_accuracy: 0.82237 |  0:10:12s\n",
      "epoch 77 | loss: 0.31024 | val_0_accuracy: 0.84671 |  0:10:20s\n",
      "epoch 78 | loss: 0.30169 | val_0_accuracy: 0.85786 |  0:10:28s\n",
      "epoch 79 | loss: 0.29831 | val_0_accuracy: 0.85032 |  0:10:36s\n",
      "epoch 80 | loss: 0.29973 | val_0_accuracy: 0.8621  |  0:10:44s\n",
      "epoch 81 | loss: 0.31264 | val_0_accuracy: 0.82378 |  0:10:52s\n",
      "epoch 82 | loss: 0.30929 | val_0_accuracy: 0.84168 |  0:11:00s\n",
      "epoch 83 | loss: 0.31014 | val_0_accuracy: 0.84184 |  0:11:07s\n",
      "epoch 84 | loss: 0.32987 | val_0_accuracy: 0.82833 |  0:11:15s\n",
      "epoch 85 | loss: 0.32056 | val_0_accuracy: 0.83666 |  0:11:23s\n",
      "epoch 86 | loss: 0.32893 | val_0_accuracy: 0.83226 |  0:11:31s\n",
      "epoch 87 | loss: 0.31396 | val_0_accuracy: 0.84891 |  0:11:39s\n",
      "epoch 88 | loss: 0.28751 | val_0_accuracy: 0.84514 |  0:11:47s\n",
      "epoch 89 | loss: 0.30341 | val_0_accuracy: 0.85629 |  0:11:55s\n",
      "epoch 90 | loss: 0.30916 | val_0_accuracy: 0.82928 |  0:12:03s\n",
      "epoch 91 | loss: 0.31775 | val_0_accuracy: 0.80823 |  0:12:11s\n",
      "epoch 92 | loss: 0.32379 | val_0_accuracy: 0.84655 |  0:12:18s\n",
      "epoch 93 | loss: 0.29741 | val_0_accuracy: 0.85503 |  0:12:26s\n",
      "epoch 94 | loss: 0.2888  | val_0_accuracy: 0.86242 |  0:12:34s\n",
      "epoch 95 | loss: 0.27724 | val_0_accuracy: 0.86163 |  0:12:42s\n",
      "epoch 96 | loss: 0.28287 | val_0_accuracy: 0.85613 |  0:12:50s\n",
      "epoch 97 | loss: 0.30321 | val_0_accuracy: 0.84545 |  0:12:58s\n",
      "epoch 98 | loss: 0.30229 | val_0_accuracy: 0.84875 |  0:13:06s\n",
      "epoch 99 | loss: 0.29988 | val_0_accuracy: 0.84907 |  0:13:13s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_val_0_accuracy = 0.86242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: copulagan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "y_train [0 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.13678 | val_0_accuracy: 0.23877 |  0:00:07s\n",
      "epoch 1  | loss: 0.73507 | val_0_accuracy: 0.18897 |  0:00:15s\n",
      "epoch 2  | loss: 0.67631 | val_0_accuracy: 0.31307 |  0:00:23s\n",
      "epoch 3  | loss: 0.64924 | val_0_accuracy: 0.19227 |  0:00:31s\n",
      "epoch 4  | loss: 0.62814 | val_0_accuracy: 0.25935 |  0:00:39s\n",
      "epoch 5  | loss: 0.63045 | val_0_accuracy: 0.33161 |  0:00:47s\n",
      "epoch 6  | loss: 0.61064 | val_0_accuracy: 0.29061 |  0:00:55s\n",
      "epoch 7  | loss: 0.58718 | val_0_accuracy: 0.35061 |  0:01:03s\n",
      "epoch 8  | loss: 0.56563 | val_0_accuracy: 0.36899 |  0:01:10s\n",
      "epoch 9  | loss: 0.55673 | val_0_accuracy: 0.29139 |  0:01:18s\n",
      "epoch 10 | loss: 0.5564  | val_0_accuracy: 0.40355 |  0:01:26s\n",
      "epoch 11 | loss: 0.5559  | val_0_accuracy: 0.37857 |  0:01:34s\n",
      "epoch 12 | loss: 0.55463 | val_0_accuracy: 0.34998 |  0:01:42s\n",
      "epoch 13 | loss: 0.55591 | val_0_accuracy: 0.31982 |  0:01:50s\n",
      "epoch 14 | loss: 0.54217 | val_0_accuracy: 0.42837 |  0:01:58s\n",
      "epoch 15 | loss: 0.54141 | val_0_accuracy: 0.37904 |  0:02:05s\n",
      "epoch 16 | loss: 0.53602 | val_0_accuracy: 0.4601  |  0:02:13s\n",
      "epoch 17 | loss: 0.52765 | val_0_accuracy: 0.46466 |  0:02:21s\n",
      "epoch 18 | loss: 0.51744 | val_0_accuracy: 0.511   |  0:02:29s\n",
      "epoch 19 | loss: 0.51606 | val_0_accuracy: 0.59142 |  0:02:37s\n",
      "epoch 20 | loss: 0.52186 | val_0_accuracy: 0.582   |  0:02:45s\n",
      "epoch 21 | loss: 0.51125 | val_0_accuracy: 0.64012 |  0:02:53s\n",
      "epoch 22 | loss: 0.5056  | val_0_accuracy: 0.61216 |  0:03:01s\n",
      "epoch 23 | loss: 0.50085 | val_0_accuracy: 0.67845 |  0:03:08s\n",
      "epoch 24 | loss: 0.4975  | val_0_accuracy: 0.72981 |  0:03:16s\n",
      "epoch 25 | loss: 0.49371 | val_0_accuracy: 0.73437 |  0:03:24s\n",
      "epoch 26 | loss: 0.49311 | val_0_accuracy: 0.74175 |  0:03:32s\n",
      "epoch 27 | loss: 0.48915 | val_0_accuracy: 0.74034 |  0:03:40s\n",
      "epoch 28 | loss: 0.49284 | val_0_accuracy: 0.72385 |  0:03:47s\n",
      "epoch 29 | loss: 0.49242 | val_0_accuracy: 0.74914 |  0:03:55s\n",
      "epoch 30 | loss: 0.48797 | val_0_accuracy: 0.75039 |  0:04:03s\n",
      "epoch 31 | loss: 0.48743 | val_0_accuracy: 0.74128 |  0:04:11s\n",
      "epoch 32 | loss: 0.48337 | val_0_accuracy: 0.76767 |  0:04:19s\n",
      "epoch 33 | loss: 0.49137 | val_0_accuracy: 0.76233 |  0:04:27s\n",
      "epoch 34 | loss: 0.50217 | val_0_accuracy: 0.78652 |  0:04:34s\n",
      "epoch 35 | loss: 0.49737 | val_0_accuracy: 0.78542 |  0:04:42s\n",
      "epoch 36 | loss: 0.49143 | val_0_accuracy: 0.78794 |  0:04:50s\n",
      "epoch 37 | loss: 0.48881 | val_0_accuracy: 0.79296 |  0:04:58s\n",
      "epoch 38 | loss: 0.48546 | val_0_accuracy: 0.80317 |  0:05:06s\n",
      "epoch 39 | loss: 0.48604 | val_0_accuracy: 0.80396 |  0:05:14s\n",
      "epoch 40 | loss: 0.48394 | val_0_accuracy: 0.80914 |  0:05:21s\n",
      "epoch 41 | loss: 0.48126 | val_0_accuracy: 0.80097 |  0:05:29s\n",
      "epoch 42 | loss: 0.48302 | val_0_accuracy: 0.77898 |  0:05:37s\n",
      "epoch 43 | loss: 0.49736 | val_0_accuracy: 0.79092 |  0:05:45s\n",
      "epoch 44 | loss: 0.50172 | val_0_accuracy: 0.78432 |  0:05:53s\n",
      "epoch 45 | loss: 0.49451 | val_0_accuracy: 0.78825 |  0:06:00s\n",
      "epoch 46 | loss: 0.48698 | val_0_accuracy: 0.80412 |  0:06:08s\n",
      "epoch 47 | loss: 0.47688 | val_0_accuracy: 0.79705 |  0:06:16s\n",
      "epoch 48 | loss: 0.48402 | val_0_accuracy: 0.77034 |  0:06:24s\n",
      "epoch 49 | loss: 0.47447 | val_0_accuracy: 0.78055 |  0:06:32s\n",
      "epoch 50 | loss: 0.46562 | val_0_accuracy: 0.79579 |  0:06:40s\n",
      "epoch 51 | loss: 0.46585 | val_0_accuracy: 0.79846 |  0:06:47s\n",
      "epoch 52 | loss: 0.45474 | val_0_accuracy: 0.80003 |  0:06:55s\n",
      "epoch 53 | loss: 0.45333 | val_0_accuracy: 0.79202 |  0:07:03s\n",
      "epoch 54 | loss: 0.45074 | val_0_accuracy: 0.77333 |  0:07:11s\n",
      "epoch 55 | loss: 0.44134 | val_0_accuracy: 0.60478 |  0:07:19s\n",
      "epoch 56 | loss: 0.42704 | val_0_accuracy: 0.56833 |  0:07:27s\n",
      "epoch 57 | loss: 0.4237  | val_0_accuracy: 0.59205 |  0:07:34s\n",
      "epoch 58 | loss: 0.4203  | val_0_accuracy: 0.52545 |  0:07:42s\n",
      "epoch 59 | loss: 0.42677 | val_0_accuracy: 0.5388  |  0:07:50s\n",
      "epoch 60 | loss: 0.42152 | val_0_accuracy: 0.76029 |  0:07:58s\n",
      "epoch 61 | loss: 0.39682 | val_0_accuracy: 0.49434 |  0:08:06s\n",
      "epoch 62 | loss: 0.38269 | val_0_accuracy: 0.48272 |  0:08:13s\n",
      "epoch 63 | loss: 0.39685 | val_0_accuracy: 0.77458 |  0:08:21s\n",
      "epoch 64 | loss: 0.37357 | val_0_accuracy: 0.71756 |  0:08:29s\n",
      "epoch 65 | loss: 0.35713 | val_0_accuracy: 0.39538 |  0:08:37s\n",
      "epoch 66 | loss: 0.35374 | val_0_accuracy: 0.78197 |  0:08:45s\n",
      "epoch 67 | loss: 0.34711 | val_0_accuracy: 0.81244 |  0:08:52s\n",
      "epoch 68 | loss: 0.34192 | val_0_accuracy: 0.71395 |  0:09:00s\n",
      "epoch 69 | loss: 0.34513 | val_0_accuracy: 0.64326 |  0:09:08s\n",
      "epoch 70 | loss: 0.32667 | val_0_accuracy: 0.87292 |  0:09:16s\n",
      "epoch 71 | loss: 0.3251  | val_0_accuracy: 0.41926 |  0:09:24s\n",
      "epoch 72 | loss: 0.34227 | val_0_accuracy: 0.39805 |  0:09:31s\n",
      "epoch 73 | loss: 0.33515 | val_0_accuracy: 0.79249 |  0:09:39s\n",
      "epoch 74 | loss: 0.34193 | val_0_accuracy: 0.82359 |  0:09:47s\n",
      "epoch 75 | loss: 0.33269 | val_0_accuracy: 0.35721 |  0:09:55s\n",
      "epoch 76 | loss: 0.31936 | val_0_accuracy: 0.67719 |  0:10:03s\n",
      "epoch 77 | loss: 0.3253  | val_0_accuracy: 0.61986 |  0:10:11s\n",
      "epoch 78 | loss: 0.31734 | val_0_accuracy: 0.64515 |  0:10:18s\n",
      "epoch 79 | loss: 0.31113 | val_0_accuracy: 0.69431 |  0:10:26s\n",
      "epoch 80 | loss: 0.31625 | val_0_accuracy: 0.65379 |  0:10:34s\n",
      "epoch 81 | loss: 0.30449 | val_0_accuracy: 0.87795 |  0:10:42s\n",
      "epoch 82 | loss: 0.30289 | val_0_accuracy: 0.66855 |  0:10:50s\n",
      "epoch 83 | loss: 0.2913  | val_0_accuracy: 0.52718 |  0:10:58s\n",
      "epoch 84 | loss: 0.29119 | val_0_accuracy: 0.59095 |  0:11:05s\n",
      "epoch 85 | loss: 0.30978 | val_0_accuracy: 0.87339 |  0:11:13s\n",
      "epoch 86 | loss: 0.28949 | val_0_accuracy: 0.49497 |  0:11:21s\n",
      "epoch 87 | loss: 0.28279 | val_0_accuracy: 0.88407 |  0:11:29s\n",
      "epoch 88 | loss: 0.28308 | val_0_accuracy: 0.50173 |  0:11:37s\n",
      "epoch 89 | loss: 0.28182 | val_0_accuracy: 0.45303 |  0:11:45s\n",
      "epoch 90 | loss: 0.28122 | val_0_accuracy: 0.47707 |  0:11:53s\n",
      "epoch 91 | loss: 0.29236 | val_0_accuracy: 0.39554 |  0:12:01s\n",
      "epoch 92 | loss: 0.2977  | val_0_accuracy: 0.45287 |  0:12:09s\n",
      "epoch 93 | loss: 0.28756 | val_0_accuracy: 0.41345 |  0:12:16s\n",
      "epoch 94 | loss: 0.28478 | val_0_accuracy: 0.88109 |  0:12:24s\n",
      "epoch 95 | loss: 0.27991 | val_0_accuracy: 0.54367 |  0:12:32s\n",
      "epoch 96 | loss: 0.27362 | val_0_accuracy: 0.88611 |  0:12:40s\n",
      "epoch 97 | loss: 0.28019 | val_0_accuracy: 0.34511 |  0:12:48s\n",
      "epoch 98 | loss: 0.28626 | val_0_accuracy: 0.34449 |  0:12:56s\n",
      "epoch 99 | loss: 0.27849 | val_0_accuracy: 0.35423 |  0:13:03s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 96 and best_val_0_accuracy = 0.88611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: ctgan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "y_train [0 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.01543 | val_0_accuracy: 0.5721  |  0:00:07s\n",
      "epoch 1  | loss: 0.68495 | val_0_accuracy: 0.43544 |  0:00:15s\n",
      "epoch 2  | loss: 0.61641 | val_0_accuracy: 0.61907 |  0:00:23s\n",
      "epoch 3  | loss: 0.58577 | val_0_accuracy: 0.65881 |  0:00:31s\n",
      "epoch 4  | loss: 0.56582 | val_0_accuracy: 0.65756 |  0:00:39s\n",
      "epoch 5  | loss: 0.55257 | val_0_accuracy: 0.65473 |  0:00:47s\n",
      "epoch 6  | loss: 0.55101 | val_0_accuracy: 0.64876 |  0:00:55s\n",
      "epoch 7  | loss: 0.53574 | val_0_accuracy: 0.63996 |  0:01:02s\n",
      "epoch 8  | loss: 0.52098 | val_0_accuracy: 0.55985 |  0:01:10s\n",
      "epoch 9  | loss: 0.50192 | val_0_accuracy: 0.50581 |  0:01:18s\n",
      "epoch 10 | loss: 0.47482 | val_0_accuracy: 0.4967  |  0:01:26s\n",
      "epoch 11 | loss: 0.45561 | val_0_accuracy: 0.44329 |  0:01:34s\n",
      "epoch 12 | loss: 0.4395  | val_0_accuracy: 0.23531 |  0:01:42s\n",
      "epoch 13 | loss: 0.41715 | val_0_accuracy: 0.39664 |  0:01:50s\n",
      "epoch 14 | loss: 0.40362 | val_0_accuracy: 0.33883 |  0:01:58s\n",
      "epoch 15 | loss: 0.3969  | val_0_accuracy: 0.25825 |  0:02:06s\n",
      "epoch 16 | loss: 0.38582 | val_0_accuracy: 0.38156 |  0:02:13s\n",
      "epoch 17 | loss: 0.38274 | val_0_accuracy: 0.35438 |  0:02:21s\n",
      "epoch 18 | loss: 0.37751 | val_0_accuracy: 0.4169  |  0:02:29s\n",
      "epoch 19 | loss: 0.37458 | val_0_accuracy: 0.40324 |  0:02:37s\n",
      "epoch 20 | loss: 0.37179 | val_0_accuracy: 0.34779 |  0:02:45s\n",
      "epoch 21 | loss: 0.3774  | val_0_accuracy: 0.48869 |  0:02:53s\n",
      "epoch 22 | loss: 0.37354 | val_0_accuracy: 0.38313 |  0:03:01s\n",
      "epoch 23 | loss: 0.3589  | val_0_accuracy: 0.42004 |  0:03:08s\n",
      "epoch 24 | loss: 0.35666 | val_0_accuracy: 0.4356  |  0:03:16s\n",
      "epoch 25 | loss: 0.34485 | val_0_accuracy: 0.42036 |  0:03:24s\n",
      "epoch 26 | loss: 0.34339 | val_0_accuracy: 0.41329 |  0:03:32s\n",
      "epoch 27 | loss: 0.34118 | val_0_accuracy: 0.43779 |  0:03:40s\n",
      "epoch 28 | loss: 0.34081 | val_0_accuracy: 0.42381 |  0:03:48s\n",
      "epoch 29 | loss: 0.34129 | val_0_accuracy: 0.3924  |  0:03:56s\n",
      "epoch 30 | loss: 0.34661 | val_0_accuracy: 0.37386 |  0:04:04s\n",
      "epoch 31 | loss: 0.34389 | val_0_accuracy: 0.34025 |  0:04:11s\n",
      "epoch 32 | loss: 0.33426 | val_0_accuracy: 0.34716 |  0:04:19s\n",
      "epoch 33 | loss: 0.3348  | val_0_accuracy: 0.37386 |  0:04:27s\n",
      "epoch 34 | loss: 0.33732 | val_0_accuracy: 0.35438 |  0:04:35s\n",
      "epoch 35 | loss: 0.34025 | val_0_accuracy: 0.33255 |  0:04:43s\n",
      "epoch 36 | loss: 0.33869 | val_0_accuracy: 0.35862 |  0:04:51s\n",
      "epoch 37 | loss: 0.33769 | val_0_accuracy: 0.43999 |  0:04:59s\n",
      "epoch 38 | loss: 0.35677 | val_0_accuracy: 0.48335 |  0:05:07s\n",
      "epoch 39 | loss: 0.34127 | val_0_accuracy: 0.48712 |  0:05:15s\n",
      "epoch 40 | loss: 0.33083 | val_0_accuracy: 0.4502  |  0:05:22s\n",
      "epoch 41 | loss: 0.32437 | val_0_accuracy: 0.54587 |  0:05:30s\n",
      "epoch 42 | loss: 0.3206  | val_0_accuracy: 0.53032 |  0:05:39s\n",
      "epoch 43 | loss: 0.31986 | val_0_accuracy: 0.74003 |  0:05:46s\n",
      "epoch 44 | loss: 0.32451 | val_0_accuracy: 0.73971 |  0:05:54s\n",
      "epoch 45 | loss: 0.31824 | val_0_accuracy: 0.54147 |  0:06:02s\n",
      "epoch 46 | loss: 0.31632 | val_0_accuracy: 0.76265 |  0:06:10s\n",
      "epoch 47 | loss: 0.31319 | val_0_accuracy: 0.88501 |  0:06:18s\n",
      "epoch 48 | loss: 0.31006 | val_0_accuracy: 0.88077 |  0:06:26s\n",
      "epoch 49 | loss: 0.31249 | val_0_accuracy: 0.67248 |  0:06:33s\n",
      "epoch 50 | loss: 0.31598 | val_0_accuracy: 0.42821 |  0:06:41s\n",
      "epoch 51 | loss: 0.31476 | val_0_accuracy: 0.4623  |  0:06:49s\n",
      "epoch 52 | loss: 0.31405 | val_0_accuracy: 0.46466 |  0:06:57s\n",
      "epoch 53 | loss: 0.31069 | val_0_accuracy: 0.88156 |  0:07:05s\n",
      "epoch 54 | loss: 0.30948 | val_0_accuracy: 0.48555 |  0:07:13s\n",
      "epoch 55 | loss: 0.3114  | val_0_accuracy: 0.38988 |  0:07:21s\n",
      "epoch 56 | loss: 0.3091  | val_0_accuracy: 0.47769 |  0:07:28s\n",
      "epoch 57 | loss: 0.30785 | val_0_accuracy: 0.51602 |  0:07:36s\n",
      "epoch 58 | loss: 0.31083 | val_0_accuracy: 0.53283 |  0:07:44s\n",
      "epoch 59 | loss: 0.30858 | val_0_accuracy: 0.45806 |  0:07:52s\n",
      "epoch 60 | loss: 0.30815 | val_0_accuracy: 0.877   |  0:08:00s\n",
      "epoch 61 | loss: 0.31206 | val_0_accuracy: 0.87983 |  0:08:08s\n",
      "epoch 62 | loss: 0.30878 | val_0_accuracy: 0.68772 |  0:08:16s\n",
      "epoch 63 | loss: 0.31215 | val_0_accuracy: 0.4689  |  0:08:24s\n",
      "epoch 64 | loss: 0.31128 | val_0_accuracy: 0.4213  |  0:08:32s\n",
      "epoch 65 | loss: 0.31234 | val_0_accuracy: 0.38674 |  0:08:40s\n",
      "epoch 66 | loss: 0.31558 | val_0_accuracy: 0.541   |  0:08:48s\n",
      "epoch 67 | loss: 0.3131  | val_0_accuracy: 0.63274 |  0:08:56s\n",
      "epoch 68 | loss: 0.3091  | val_0_accuracy: 0.56032 |  0:09:03s\n",
      "epoch 69 | loss: 0.30654 | val_0_accuracy: 0.56126 |  0:09:11s\n",
      "epoch 70 | loss: 0.30481 | val_0_accuracy: 0.52922 |  0:09:19s\n",
      "epoch 71 | loss: 0.30609 | val_0_accuracy: 0.48492 |  0:09:27s\n",
      "epoch 72 | loss: 0.3131  | val_0_accuracy: 0.50691 |  0:09:35s\n",
      "epoch 73 | loss: 0.30977 | val_0_accuracy: 0.6907  |  0:09:42s\n",
      "epoch 74 | loss: 0.30701 | val_0_accuracy: 0.62441 |  0:09:50s\n",
      "epoch 75 | loss: 0.30506 | val_0_accuracy: 0.41093 |  0:09:58s\n",
      "epoch 76 | loss: 0.30133 | val_0_accuracy: 0.42413 |  0:10:06s\n",
      "epoch 77 | loss: 0.29837 | val_0_accuracy: 0.44471 |  0:10:14s\n",
      "epoch 78 | loss: 0.30054 | val_0_accuracy: 0.52529 |  0:10:22s\n",
      "epoch 79 | loss: 0.30424 | val_0_accuracy: 0.3968  |  0:10:30s\n",
      "epoch 80 | loss: 0.29985 | val_0_accuracy: 0.49325 |  0:10:38s\n",
      "epoch 81 | loss: 0.29771 | val_0_accuracy: 0.86978 |  0:10:45s\n",
      "epoch 82 | loss: 0.29738 | val_0_accuracy: 0.45397 |  0:10:53s\n",
      "epoch 83 | loss: 0.29622 | val_0_accuracy: 0.5044  |  0:11:01s\n",
      "epoch 84 | loss: 0.29388 | val_0_accuracy: 0.56095 |  0:11:09s\n",
      "epoch 85 | loss: 0.29264 | val_0_accuracy: 0.57194 |  0:11:17s\n",
      "epoch 86 | loss: 0.29307 | val_0_accuracy: 0.88753 |  0:11:25s\n",
      "epoch 87 | loss: 0.29161 | val_0_accuracy: 0.55734 |  0:11:33s\n",
      "epoch 88 | loss: 0.29092 | val_0_accuracy: 0.89036 |  0:11:41s\n",
      "epoch 89 | loss: 0.2902  | val_0_accuracy: 0.78385 |  0:11:49s\n",
      "epoch 90 | loss: 0.29115 | val_0_accuracy: 0.5509  |  0:11:57s\n",
      "epoch 91 | loss: 0.2901  | val_0_accuracy: 0.50785 |  0:12:04s\n",
      "epoch 92 | loss: 0.2879  | val_0_accuracy: 0.50848 |  0:12:12s\n",
      "epoch 93 | loss: 0.28611 | val_0_accuracy: 0.53691 |  0:12:20s\n",
      "epoch 94 | loss: 0.28618 | val_0_accuracy: 0.5743  |  0:12:28s\n",
      "epoch 95 | loss: 0.28408 | val_0_accuracy: 0.59755 |  0:12:36s\n",
      "epoch 96 | loss: 0.28496 | val_0_accuracy: 0.59441 |  0:12:43s\n",
      "epoch 97 | loss: 0.2849  | val_0_accuracy: 0.87732 |  0:12:51s\n",
      "epoch 98 | loss: 0.28393 | val_0_accuracy: 0.5743  |  0:12:59s\n",
      "epoch 99 | loss: 0.28173 | val_0_accuracy: 0.60383 |  0:13:07s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 88 and best_val_0_accuracy = 0.89036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: nbsynthetic\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "y_train [0 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.92091 | val_0_accuracy: 0.65551 |  0:00:07s\n",
      "epoch 1  | loss: 0.25601 | val_0_accuracy: 0.65379 |  0:00:15s\n",
      "epoch 2  | loss: 0.19803 | val_0_accuracy: 0.23657 |  0:00:23s\n",
      "epoch 3  | loss: 0.17614 | val_0_accuracy: 0.55639 |  0:00:31s\n",
      "epoch 4  | loss: 0.16425 | val_0_accuracy: 0.63132 |  0:00:39s\n",
      "epoch 5  | loss: 0.15749 | val_0_accuracy: 0.72212 |  0:00:47s\n",
      "epoch 6  | loss: 0.151   | val_0_accuracy: 0.72432 |  0:00:54s\n",
      "epoch 7  | loss: 0.14554 | val_0_accuracy: 0.67279 |  0:01:02s\n",
      "epoch 8  | loss: 0.14357 | val_0_accuracy: 0.70264 |  0:01:10s\n",
      "epoch 9  | loss: 0.14272 | val_0_accuracy: 0.67908 |  0:01:18s\n",
      "epoch 10 | loss: 0.1406  | val_0_accuracy: 0.74866 |  0:01:26s\n",
      "epoch 11 | loss: 0.13945 | val_0_accuracy: 0.82187 |  0:01:33s\n",
      "epoch 12 | loss: 0.13821 | val_0_accuracy: 0.84559 |  0:01:41s\n",
      "epoch 13 | loss: 0.1358  | val_0_accuracy: 0.84998 |  0:01:49s\n",
      "epoch 14 | loss: 0.14007 | val_0_accuracy: 0.80679 |  0:01:57s\n",
      "epoch 15 | loss: 0.14184 | val_0_accuracy: 0.83585 |  0:02:05s\n",
      "epoch 16 | loss: 0.13848 | val_0_accuracy: 0.86836 |  0:02:12s\n",
      "epoch 17 | loss: 0.12983 | val_0_accuracy: 0.87056 |  0:02:20s\n",
      "epoch 18 | loss: 0.12594 | val_0_accuracy: 0.84716 |  0:02:28s\n",
      "epoch 19 | loss: 0.12524 | val_0_accuracy: 0.87857 |  0:02:36s\n",
      "epoch 20 | loss: 0.12192 | val_0_accuracy: 0.86082 |  0:02:44s\n",
      "epoch 21 | loss: 0.12292 | val_0_accuracy: 0.89255 |  0:02:52s\n",
      "epoch 22 | loss: 0.1256  | val_0_accuracy: 0.93135 |  0:03:00s\n",
      "epoch 23 | loss: 0.12529 | val_0_accuracy: 0.93418 |  0:03:07s\n",
      "epoch 24 | loss: 0.12337 | val_0_accuracy: 0.92696 |  0:03:15s\n",
      "epoch 25 | loss: 0.12225 | val_0_accuracy: 0.93104 |  0:03:23s\n",
      "epoch 26 | loss: 0.12188 | val_0_accuracy: 0.93968 |  0:03:31s\n",
      "epoch 27 | loss: 0.12045 | val_0_accuracy: 0.94408 |  0:03:39s\n",
      "epoch 28 | loss: 0.11872 | val_0_accuracy: 0.9502  |  0:03:46s\n",
      "epoch 29 | loss: 0.11725 | val_0_accuracy: 0.95429 |  0:03:54s\n",
      "epoch 30 | loss: 0.1181  | val_0_accuracy: 0.95617 |  0:04:02s\n",
      "epoch 31 | loss: 0.11627 | val_0_accuracy: 0.96057 |  0:04:10s\n",
      "epoch 32 | loss: 0.11477 | val_0_accuracy: 0.96089 |  0:04:18s\n",
      "epoch 33 | loss: 0.1145  | val_0_accuracy: 0.96544 |  0:04:25s\n",
      "epoch 34 | loss: 0.11443 | val_0_accuracy: 0.96167 |  0:04:35s\n",
      "epoch 35 | loss: 0.11449 | val_0_accuracy: 0.96246 |  0:04:43s\n",
      "epoch 36 | loss: 0.11454 | val_0_accuracy: 0.95979 |  0:04:51s\n",
      "epoch 37 | loss: 0.11443 | val_0_accuracy: 0.9656  |  0:04:59s\n",
      "epoch 38 | loss: 0.11313 | val_0_accuracy: 0.95633 |  0:05:07s\n",
      "epoch 39 | loss: 0.11305 | val_0_accuracy: 0.96623 |  0:05:14s\n",
      "epoch 40 | loss: 0.11252 | val_0_accuracy: 0.96607 |  0:05:22s\n",
      "epoch 41 | loss: 0.11091 | val_0_accuracy: 0.96513 |  0:05:30s\n",
      "epoch 42 | loss: 0.11212 | val_0_accuracy: 0.96764 |  0:05:38s\n",
      "epoch 43 | loss: 0.11277 | val_0_accuracy: 0.96717 |  0:05:46s\n",
      "epoch 44 | loss: 0.11181 | val_0_accuracy: 0.96858 |  0:05:54s\n",
      "epoch 45 | loss: 0.11085 | val_0_accuracy: 0.96858 |  0:06:02s\n",
      "epoch 46 | loss: 0.11627 | val_0_accuracy: 0.96528 |  0:06:10s\n",
      "epoch 47 | loss: 0.11466 | val_0_accuracy: 0.96733 |  0:06:17s\n",
      "epoch 48 | loss: 0.11278 | val_0_accuracy: 0.96795 |  0:06:25s\n",
      "epoch 49 | loss: 0.11013 | val_0_accuracy: 0.97    |  0:06:33s\n",
      "epoch 50 | loss: 0.11039 | val_0_accuracy: 0.97    |  0:06:41s\n",
      "epoch 51 | loss: 0.11017 | val_0_accuracy: 0.96623 |  0:06:49s\n",
      "epoch 52 | loss: 0.11193 | val_0_accuracy: 0.97063 |  0:06:56s\n",
      "epoch 53 | loss: 0.11069 | val_0_accuracy: 0.96638 |  0:07:04s\n",
      "epoch 54 | loss: 0.111   | val_0_accuracy: 0.96811 |  0:07:12s\n",
      "epoch 55 | loss: 0.11161 | val_0_accuracy: 0.97015 |  0:07:20s\n",
      "epoch 56 | loss: 0.11166 | val_0_accuracy: 0.96953 |  0:07:28s\n",
      "epoch 57 | loss: 0.10912 | val_0_accuracy: 0.96953 |  0:07:36s\n",
      "epoch 58 | loss: 0.1081  | val_0_accuracy: 0.96905 |  0:07:44s\n",
      "epoch 59 | loss: 0.10942 | val_0_accuracy: 0.9689  |  0:07:52s\n",
      "epoch 60 | loss: 0.10728 | val_0_accuracy: 0.9689  |  0:07:59s\n",
      "epoch 61 | loss: 0.10882 | val_0_accuracy: 0.96937 |  0:08:07s\n",
      "epoch 62 | loss: 0.10666 | val_0_accuracy: 0.96921 |  0:08:15s\n",
      "epoch 63 | loss: 0.10908 | val_0_accuracy: 0.96686 |  0:08:23s\n",
      "epoch 64 | loss: 0.10732 | val_0_accuracy: 0.97063 |  0:08:31s\n",
      "epoch 65 | loss: 0.10698 | val_0_accuracy: 0.97094 |  0:08:39s\n",
      "epoch 66 | loss: 0.10523 | val_0_accuracy: 0.96764 |  0:08:47s\n",
      "epoch 67 | loss: 0.10332 | val_0_accuracy: 0.97063 |  0:08:55s\n",
      "epoch 68 | loss: 0.10243 | val_0_accuracy: 0.96968 |  0:09:03s\n",
      "epoch 69 | loss: 0.10944 | val_0_accuracy: 0.96293 |  0:09:10s\n",
      "epoch 70 | loss: 0.11159 | val_0_accuracy: 0.96497 |  0:09:18s\n",
      "epoch 71 | loss: 0.1102  | val_0_accuracy: 0.96623 |  0:09:26s\n",
      "epoch 72 | loss: 0.11293 | val_0_accuracy: 0.96827 |  0:09:34s\n",
      "epoch 73 | loss: 0.10872 | val_0_accuracy: 0.9678  |  0:09:42s\n",
      "epoch 74 | loss: 0.1089  | val_0_accuracy: 0.96921 |  0:09:50s\n",
      "epoch 75 | loss: 0.1057  | val_0_accuracy: 0.97015 |  0:09:58s\n",
      "epoch 76 | loss: 0.10191 | val_0_accuracy: 0.97157 |  0:10:06s\n",
      "epoch 77 | loss: 0.10292 | val_0_accuracy: 0.96968 |  0:10:14s\n",
      "epoch 78 | loss: 0.10349 | val_0_accuracy: 0.9711  |  0:10:22s\n",
      "epoch 79 | loss: 0.10204 | val_0_accuracy: 0.97141 |  0:10:29s\n",
      "epoch 80 | loss: 0.1011  | val_0_accuracy: 0.96921 |  0:10:37s\n",
      "epoch 81 | loss: 0.10147 | val_0_accuracy: 0.97235 |  0:10:45s\n",
      "epoch 82 | loss: 0.10083 | val_0_accuracy: 0.97094 |  0:10:53s\n",
      "epoch 83 | loss: 0.10111 | val_0_accuracy: 0.96905 |  0:11:01s\n",
      "epoch 84 | loss: 0.10963 | val_0_accuracy: 0.96984 |  0:11:09s\n",
      "epoch 85 | loss: 0.10683 | val_0_accuracy: 0.9667  |  0:11:17s\n",
      "epoch 86 | loss: 0.1098  | val_0_accuracy: 0.96591 |  0:11:25s\n",
      "epoch 87 | loss: 0.11014 | val_0_accuracy: 0.96497 |  0:11:32s\n",
      "epoch 88 | loss: 0.10718 | val_0_accuracy: 0.96481 |  0:11:40s\n",
      "epoch 89 | loss: 0.11295 | val_0_accuracy: 0.96686 |  0:11:48s\n",
      "epoch 90 | loss: 0.11013 | val_0_accuracy: 0.96858 |  0:11:56s\n",
      "epoch 91 | loss: 0.10825 | val_0_accuracy: 0.96874 |  0:12:04s\n",
      "epoch 92 | loss: 0.10764 | val_0_accuracy: 0.97031 |  0:12:12s\n",
      "epoch 93 | loss: 0.11194 | val_0_accuracy: 0.9667  |  0:12:20s\n",
      "epoch 94 | loss: 0.10899 | val_0_accuracy: 0.96387 |  0:12:28s\n",
      "epoch 95 | loss: 0.12797 | val_0_accuracy: 0.96183 |  0:12:36s\n",
      "epoch 96 | loss: 0.11875 | val_0_accuracy: 0.96277 |  0:12:44s\n",
      "epoch 97 | loss: 0.11542 | val_0_accuracy: 0.96481 |  0:12:51s\n",
      "epoch 98 | loss: 0.11193 | val_0_accuracy: 0.96701 |  0:12:59s\n",
      "epoch 99 | loss: 0.11401 | val_0_accuracy: 0.96199 |  0:13:07s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 81 and best_val_0_accuracy = 0.97235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    print(f\"\\n처리 중인 생성 모델: {generation}\")\n",
    "        \n",
    "    # 1. 데이터 전처리\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = preprocess_data(generation)\n",
    "    \n",
    "    print(\"y_train\",np.unique(y_train))\n",
    "    # 2. TabNet 모델 학습\n",
    "    tabnet_clf = train_tabnet_classifier(X_train, y_train, X_valid, y_valid)\n",
    "    \n",
    "\n",
    "    # 3. 모델 저장\n",
    "    model_save_path = path+f'\\\\model_hist\\\\{generation}_proposed_tabnet_model.pickle'\n",
    "    save_model(tabnet_clf, model_save_path)\n",
    "    \n",
    "    # 4. 예측\n",
    "    predict = tabnet_clf.predict(X_test)\n",
    "    \n",
    "    \n",
    "    # 각 메트릭 계산\n",
    "    accuracy = accuracy_score(y_test, predict)\n",
    "    f1 = f1_score(y_test, predict, average='weighted')\n",
    "    precision, recall = sk(y_test, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-TabNet\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_TabNet_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT_model = DecisionTreeClassifier(\n",
    "     max_depth=6,\n",
    "    min_samples_leaf=20,  # 노드를 분할하기 위한 최소 샘플 수를 증가\n",
    "    random_state=42,\n",
    "    max_features=1    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: smote\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: adasyn\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: copulagan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: ctgan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: nbsynthetic\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    print(f\"\\n처리 중인 생성 모델: {generation}\")\n",
    "        \n",
    "    # 1. 데이터 전처리\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = preprocess_data(generation)\n",
    "    X_train = np.concatenate((X_train, X_valid), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_valid), axis=0)\n",
    " \n",
    "    DT_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3. 모델 저장\n",
    "    model_save_path = path+f'\\\\model_hist\\\\{generation}_DT_model.pickle'\n",
    "    save_model(DT_model, model_save_path)\n",
    "    \n",
    "    \n",
    "    predict = DT_model.predict(X_test)\n",
    "    \n",
    "    # 각 메트릭 계산\n",
    "    accuracy = accuracy_score(y_test, predict)\n",
    "    f1 = f1_score(y_test, predict, average='weighted')\n",
    "    precision, recall = sk(y_test, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-Decision Tree\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_DT_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-Decision Tree</td>\n",
       "      <td>0.70109</td>\n",
       "      <td>0.66712</td>\n",
       "      <td>0.66591</td>\n",
       "      <td>0.70109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-Decision Tree</td>\n",
       "      <td>0.71121</td>\n",
       "      <td>0.66591</td>\n",
       "      <td>0.68470</td>\n",
       "      <td>0.71121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-Decision Tree</td>\n",
       "      <td>0.81410</td>\n",
       "      <td>0.80762</td>\n",
       "      <td>0.80844</td>\n",
       "      <td>0.81410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-Decision Tree</td>\n",
       "      <td>0.78939</td>\n",
       "      <td>0.77514</td>\n",
       "      <td>0.77645</td>\n",
       "      <td>0.78939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-Decision Tree</td>\n",
       "      <td>0.94601</td>\n",
       "      <td>0.94464</td>\n",
       "      <td>0.94484</td>\n",
       "      <td>0.94601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-Decision Tree   0.70109  0.66712    0.66591  0.70109\n",
       "1       adasyn-Decision Tree   0.71121  0.66591    0.68470  0.71121\n",
       "2    copulagan-Decision Tree   0.81410  0.80762    0.80844  0.81410\n",
       "3        ctgan-Decision Tree   0.78939  0.77514    0.77645  0.78939\n",
       "4  nbsynthetic-Decision Tree   0.94601  0.94464    0.94484  0.94601"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_DT_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: smote\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: adasyn\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: copulagan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: ctgan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: nbsynthetic\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    print(f\"\\n처리 중인 생성 모델: {generation}\")\n",
    "        \n",
    "    # 1. 데이터 전처리\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = preprocess_data(generation)\n",
    "    X_train = np.concatenate((X_train, X_valid), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_valid), axis=0)\n",
    " \n",
    "    NB_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3. 모델 저장\n",
    "    model_save_path = path+f'\\\\model_hist\\\\{generation}_NB_model.pickle'\n",
    "    save_model(NB_model, model_save_path)\n",
    "    \n",
    "    \n",
    "    predict = NB_model.predict(X_test)\n",
    "    \n",
    "    # 각 메트릭 계산\n",
    "    accuracy = accuracy_score(y_test, predict)\n",
    "    f1 = f1_score(y_test, predict, average='weighted')\n",
    "    precision, recall = sk(y_test, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-Naive Bayes\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_Naive_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-Naive Bayes</td>\n",
       "      <td>0.65906</td>\n",
       "      <td>0.57410</td>\n",
       "      <td>0.56929</td>\n",
       "      <td>0.65906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-Naive Bayes</td>\n",
       "      <td>0.67112</td>\n",
       "      <td>0.59540</td>\n",
       "      <td>0.60596</td>\n",
       "      <td>0.67112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-Naive Bayes</td>\n",
       "      <td>0.68928</td>\n",
       "      <td>0.64308</td>\n",
       "      <td>0.65313</td>\n",
       "      <td>0.68928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-Naive Bayes</td>\n",
       "      <td>0.59232</td>\n",
       "      <td>0.58701</td>\n",
       "      <td>0.62358</td>\n",
       "      <td>0.59232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-Naive Bayes</td>\n",
       "      <td>0.90571</td>\n",
       "      <td>0.90391</td>\n",
       "      <td>0.90366</td>\n",
       "      <td>0.90571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-Naive Bayes   0.65906  0.57410    0.56929  0.65906\n",
       "1       adasyn-Naive Bayes   0.67112  0.59540    0.60596  0.67112\n",
       "2    copulagan-Naive Bayes   0.68928  0.64308    0.65313  0.68928\n",
       "3        ctgan-Naive Bayes   0.59232  0.58701    0.62358  0.59232\n",
       "4  nbsynthetic-Naive Bayes   0.90571  0.90391    0.90366  0.90571"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Naive_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDA_model = LinearDiscriminantAnalysis(\n",
    "    solver='lsqr', \n",
    "    shrinkage=0.9,           \n",
    "    n_components=2,           \n",
    "    tol=1e-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: smote\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: adasyn\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: copulagan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: ctgan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: nbsynthetic\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    print(f\"\\n처리 중인 생성 모델: {generation}\")\n",
    "        \n",
    "    # 1. 데이터 전처리\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = preprocess_data(generation)\n",
    "    X_train = np.concatenate((X_train, X_valid), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_valid), axis=0)\n",
    "\n",
    " \n",
    "    LDA_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3. 모델 저장\n",
    "    model_save_path = path+f'\\\\model_hist\\\\{generation}_LDA_model.pickle'\n",
    "    save_model(LDA_model, model_save_path)\n",
    "    \n",
    "    \n",
    "    predict = LDA_model.predict(X_test)\n",
    "    \n",
    "    # 각 메트릭 계산\n",
    "    accuracy = accuracy_score(y_test, predict)\n",
    "    f1 = f1_score(y_test, predict, average='weighted')\n",
    "    precision, recall = sk(y_test, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-LDA\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_LDA_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-LDA</td>\n",
       "      <td>0.64111</td>\n",
       "      <td>0.53352</td>\n",
       "      <td>0.47249</td>\n",
       "      <td>0.64111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-LDA</td>\n",
       "      <td>0.63701</td>\n",
       "      <td>0.52656</td>\n",
       "      <td>0.45768</td>\n",
       "      <td>0.63701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-LDA</td>\n",
       "      <td>0.65119</td>\n",
       "      <td>0.59334</td>\n",
       "      <td>0.54555</td>\n",
       "      <td>0.65119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-LDA</td>\n",
       "      <td>0.63639</td>\n",
       "      <td>0.57703</td>\n",
       "      <td>0.65650</td>\n",
       "      <td>0.63639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-LDA</td>\n",
       "      <td>0.69479</td>\n",
       "      <td>0.67580</td>\n",
       "      <td>0.68568</td>\n",
       "      <td>0.69479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-LDA   0.64111  0.53352    0.47249  0.64111\n",
       "1       adasyn-LDA   0.63701  0.52656    0.45768  0.63701\n",
       "2    copulagan-LDA   0.65119  0.59334    0.54555  0.65119\n",
       "3        ctgan-LDA   0.63639  0.57703    0.65650  0.63639\n",
       "4  nbsynthetic-LDA   0.69479  0.67580    0.68568  0.69479"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_LDA_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터 로드 및 전처리 (예시)\n",
    "# X 데이터는 6개의 특징 변수, y 데이터는 라벨 (1.0, 2.0, 3.0)\n",
    "# X, y 데이터는 적절히 로드되어 있어야 합니다.\n",
    "# 모델 정의\n",
    "\n",
    "Logistic_model = LogisticRegression(\n",
    "    multi_class='multinomial',\n",
    "    max_iter=10,\n",
    "    penalty='l2',         # L2 규제를 사용\n",
    "    C=2.0,                # 규제 강도를 기본값으로 조정\n",
    "    warm_start=True,     # 초기화를 재사용하지 않음\n",
    "    random_state=52,\n",
    "    tol=1e-4              # 수렴 기준을 적절한 수준으로 설정\n",
    "    , solver='lbfgs'\n",
    ")\n",
    "# Logistic_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: smote\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: adasyn\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: copulagan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: ctgan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: nbsynthetic\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    print(f\"\\n처리 중인 생성 모델: {generation}\")        \n",
    "    # 1. 데이터 전처리\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = preprocess_data(generation)\n",
    "    \n",
    "    X_train = np.concatenate((X_train, X_valid), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_valid), axis=0)\n",
    "\n",
    "    # 스케일링 적용\n",
    "    # scaler=StandardScaler()\n",
    "    # X_train=scaler.fit_transform(X_train)\n",
    "    # X_test=scaler.transform(X_test)\n",
    " \n",
    "    Logistic_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3. 모델 저장\n",
    "    model_save_path = path+f'\\\\model_hist\\\\{generation}_Logistic_model.pickle'\n",
    "    \n",
    "    with open(path+f'\\\\model_hist\\\\{generation}__Logistic_scaler.pickle', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "        \n",
    "    save_model(Logistic_model, model_save_path)\n",
    "    \n",
    "    \n",
    "    predict = Logistic_model.predict(X_test)\n",
    "    \n",
    "    # 각 메트릭 계산\n",
    "    accuracy = accuracy_score(y_test, predict)\n",
    "    f1 = f1_score(y_test, predict, average='weighted')\n",
    "    precision, recall = sk(y_test, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-LDA\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_LogisticRegression_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-LDA</td>\n",
       "      <td>0.56887</td>\n",
       "      <td>0.53406</td>\n",
       "      <td>0.52167</td>\n",
       "      <td>0.56887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-LDA</td>\n",
       "      <td>0.54425</td>\n",
       "      <td>0.54223</td>\n",
       "      <td>0.54029</td>\n",
       "      <td>0.54425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-LDA</td>\n",
       "      <td>0.54714</td>\n",
       "      <td>0.55293</td>\n",
       "      <td>0.56019</td>\n",
       "      <td>0.54714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-LDA</td>\n",
       "      <td>0.59673</td>\n",
       "      <td>0.58229</td>\n",
       "      <td>0.56993</td>\n",
       "      <td>0.59673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-LDA</td>\n",
       "      <td>0.87644</td>\n",
       "      <td>0.87562</td>\n",
       "      <td>0.87496</td>\n",
       "      <td>0.87644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-LDA   0.56887  0.53406    0.52167  0.56887\n",
       "1       adasyn-LDA   0.54425  0.54223    0.54029  0.54425\n",
       "2    copulagan-LDA   0.54714  0.55293    0.56019  0.54714\n",
       "3        ctgan-LDA   0.59673  0.58229    0.56993  0.59673\n",
       "4  nbsynthetic-LDA   0.87644  0.87562    0.87496  0.87644"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_LogisticRegression_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygam import LogisticGAM\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "base_estimator = LogisticGAM(n_splines=5)\n",
    "ensemble = OneVsRestClassifier(base_estimator, n_jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 중인 생성 모델: smote\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: adasyn\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: copulagan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: ctgan\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n",
      "\n",
      "처리 중인 생성 모델: nbsynthetic\n",
      "경로 확인 c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from pygam import LogisticGAM, s, f\n",
    "\n",
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    print(f\"\\n처리 중인 생성 모델: {generation}\")\n",
    "    \n",
    "    # 1. 데이터 전처리\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = preprocess_data(generation)\n",
    "    \n",
    "    X_train = np.concatenate((X_train, X_valid), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_valid), axis=0)\n",
    "    \n",
    "    # 레이블 인코딩\n",
    "    le = LabelEncoder()\n",
    "    y_train_encoded = le.fit_transform(y_train)\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "    n_classes = len(np.unique(y_train_encoded))\n",
    "    \n",
    "    # 각 클래스별 GAM 모델 학습 (One-vs-Rest 방식)\n",
    "    gam_models = []\n",
    "    for class_idx in range(n_classes):\n",
    "        y_binary = (y_train_encoded == class_idx).astype(int)\n",
    "        \n",
    "        formula = s(0)+s(1)+s(2)  \n",
    "        gam = LogisticGAM(formula)\n",
    "        gam.fit(X_train, y_binary)\n",
    "        gam_models.append(gam)\n",
    "    \n",
    "    # 3. 모델 저장\n",
    "    model_save_path = path+f'\\\\model_hist\\\\{generation}_GAMs_model.pickle'\n",
    "    \n",
    "    with open(model_save_path, 'wb') as f:\n",
    "        pickle.dump(gam_models, f)\n",
    "    \n",
    "    # 4. 예측 및 평가\n",
    "    # 각 클래스에 대한 예측 확률 계산\n",
    "    probs = np.column_stack([\n",
    "        gam.predict_proba(X_test) for gam in gam_models\n",
    "    ])\n",
    "    \n",
    "    # 가장 높은 확률을 가진 클래스 선택\n",
    "    predict = np.argmax(probs, axis=1)\n",
    "    \n",
    "    # 각 메트릭 계산\n",
    "    accuracy = accuracy_score(y_test_encoded, predict)\n",
    "    f1 = f1_score(y_test_encoded, predict, average='weighted')\n",
    "    precision, recall = sk(y_test_encoded, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-GAMs\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_GAMs_results = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터프레임을 CSV 파일로 저장\n",
    "# df_GAMs_results.to_csv(path + f'\\\\results\\\\GAMs_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-GAMs</td>\n",
       "      <td>0.67008</td>\n",
       "      <td>0.55214</td>\n",
       "      <td>0.63488</td>\n",
       "      <td>0.67008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-GAMs</td>\n",
       "      <td>0.67254</td>\n",
       "      <td>0.55596</td>\n",
       "      <td>0.65869</td>\n",
       "      <td>0.67254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-GAMs</td>\n",
       "      <td>0.73792</td>\n",
       "      <td>0.68200</td>\n",
       "      <td>0.76323</td>\n",
       "      <td>0.73792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-GAMs</td>\n",
       "      <td>0.72234</td>\n",
       "      <td>0.64406</td>\n",
       "      <td>0.71380</td>\n",
       "      <td>0.72234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-GAMs</td>\n",
       "      <td>0.92208</td>\n",
       "      <td>0.91817</td>\n",
       "      <td>0.91976</td>\n",
       "      <td>0.92208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-GAMs   0.67008  0.55214    0.63488  0.67008\n",
       "1       adasyn-GAMs   0.67254  0.55596    0.65869  0.67254\n",
       "2    copulagan-GAMs   0.73792  0.68200    0.76323  0.73792\n",
       "3        ctgan-GAMs   0.72234  0.64406    0.71380  0.72234\n",
       "4  nbsynthetic-GAMs   0.92208  0.91817    0.91976  0.92208"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GAMs_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 총합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-Naive Bayes</td>\n",
       "      <td>0.65906</td>\n",
       "      <td>0.57410</td>\n",
       "      <td>0.56929</td>\n",
       "      <td>0.65906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-Naive Bayes</td>\n",
       "      <td>0.67112</td>\n",
       "      <td>0.59540</td>\n",
       "      <td>0.60596</td>\n",
       "      <td>0.67112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-Naive Bayes</td>\n",
       "      <td>0.68928</td>\n",
       "      <td>0.64308</td>\n",
       "      <td>0.65313</td>\n",
       "      <td>0.68928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-Naive Bayes</td>\n",
       "      <td>0.59232</td>\n",
       "      <td>0.58701</td>\n",
       "      <td>0.62358</td>\n",
       "      <td>0.59232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-Naive Bayes</td>\n",
       "      <td>0.90571</td>\n",
       "      <td>0.90391</td>\n",
       "      <td>0.90366</td>\n",
       "      <td>0.90571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-Naive Bayes   0.65906  0.57410    0.56929  0.65906\n",
       "1       adasyn-Naive Bayes   0.67112  0.59540    0.60596  0.67112\n",
       "2    copulagan-Naive Bayes   0.68928  0.64308    0.65313  0.68928\n",
       "3        ctgan-Naive Bayes   0.59232  0.58701    0.62358  0.59232\n",
       "4  nbsynthetic-Naive Bayes   0.90571  0.90391    0.90366  0.90571"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Naive_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-LDA</td>\n",
       "      <td>0.64111</td>\n",
       "      <td>0.53352</td>\n",
       "      <td>0.47249</td>\n",
       "      <td>0.64111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-LDA</td>\n",
       "      <td>0.63701</td>\n",
       "      <td>0.52656</td>\n",
       "      <td>0.45768</td>\n",
       "      <td>0.63701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-LDA</td>\n",
       "      <td>0.65119</td>\n",
       "      <td>0.59334</td>\n",
       "      <td>0.54555</td>\n",
       "      <td>0.65119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-LDA</td>\n",
       "      <td>0.63639</td>\n",
       "      <td>0.57703</td>\n",
       "      <td>0.65650</td>\n",
       "      <td>0.63639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-LDA</td>\n",
       "      <td>0.69479</td>\n",
       "      <td>0.67580</td>\n",
       "      <td>0.68568</td>\n",
       "      <td>0.69479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-LDA   0.64111  0.53352    0.47249  0.64111\n",
       "1       adasyn-LDA   0.63701  0.52656    0.45768  0.63701\n",
       "2    copulagan-LDA   0.65119  0.59334    0.54555  0.65119\n",
       "3        ctgan-LDA   0.63639  0.57703    0.65650  0.63639\n",
       "4  nbsynthetic-LDA   0.69479  0.67580    0.68568  0.69479"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_LDA_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-LDA</td>\n",
       "      <td>0.56887</td>\n",
       "      <td>0.53406</td>\n",
       "      <td>0.52167</td>\n",
       "      <td>0.56887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-LDA</td>\n",
       "      <td>0.54425</td>\n",
       "      <td>0.54223</td>\n",
       "      <td>0.54029</td>\n",
       "      <td>0.54425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-LDA</td>\n",
       "      <td>0.54714</td>\n",
       "      <td>0.55293</td>\n",
       "      <td>0.56019</td>\n",
       "      <td>0.54714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-LDA</td>\n",
       "      <td>0.59673</td>\n",
       "      <td>0.58229</td>\n",
       "      <td>0.56993</td>\n",
       "      <td>0.59673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-LDA</td>\n",
       "      <td>0.87644</td>\n",
       "      <td>0.87562</td>\n",
       "      <td>0.87496</td>\n",
       "      <td>0.87644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Accuracy  F1score  Precision   Recall\n",
       "0        smote-LDA   0.56887  0.53406    0.52167  0.56887\n",
       "1       adasyn-LDA   0.54425  0.54223    0.54029  0.54425\n",
       "2    copulagan-LDA   0.54714  0.55293    0.56019  0.54714\n",
       "3        ctgan-LDA   0.59673  0.58229    0.56993  0.59673\n",
       "4  nbsynthetic-LDA   0.87644  0.87562    0.87496  0.87644"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_LogisticRegression_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml=pd.concat([df_DT_results,df_Naive_results,df_LDA_results,df_GAMs_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml=df_ml.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.to_csv(path+'\\\\data\\\\머신 러닝 통합 결과정리.csv',encoding='cp949',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smote-Decision Tree</td>\n",
       "      <td>0.70109</td>\n",
       "      <td>0.66712</td>\n",
       "      <td>0.66591</td>\n",
       "      <td>0.70109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adasyn-Decision Tree</td>\n",
       "      <td>0.71121</td>\n",
       "      <td>0.66591</td>\n",
       "      <td>0.68470</td>\n",
       "      <td>0.71121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copulagan-Decision Tree</td>\n",
       "      <td>0.81410</td>\n",
       "      <td>0.80762</td>\n",
       "      <td>0.80844</td>\n",
       "      <td>0.81410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctgan-Decision Tree</td>\n",
       "      <td>0.78939</td>\n",
       "      <td>0.77514</td>\n",
       "      <td>0.77645</td>\n",
       "      <td>0.78939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbsynthetic-Decision Tree</td>\n",
       "      <td>0.94601</td>\n",
       "      <td>0.94464</td>\n",
       "      <td>0.94484</td>\n",
       "      <td>0.94601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>smote-Naive Bayes</td>\n",
       "      <td>0.65906</td>\n",
       "      <td>0.57410</td>\n",
       "      <td>0.56929</td>\n",
       "      <td>0.65906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adasyn-Naive Bayes</td>\n",
       "      <td>0.67112</td>\n",
       "      <td>0.59540</td>\n",
       "      <td>0.60596</td>\n",
       "      <td>0.67112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>copulagan-Naive Bayes</td>\n",
       "      <td>0.68928</td>\n",
       "      <td>0.64308</td>\n",
       "      <td>0.65313</td>\n",
       "      <td>0.68928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ctgan-Naive Bayes</td>\n",
       "      <td>0.59232</td>\n",
       "      <td>0.58701</td>\n",
       "      <td>0.62358</td>\n",
       "      <td>0.59232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nbsynthetic-Naive Bayes</td>\n",
       "      <td>0.90571</td>\n",
       "      <td>0.90391</td>\n",
       "      <td>0.90366</td>\n",
       "      <td>0.90571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>smote-LDA</td>\n",
       "      <td>0.64111</td>\n",
       "      <td>0.53352</td>\n",
       "      <td>0.47249</td>\n",
       "      <td>0.64111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>adasyn-LDA</td>\n",
       "      <td>0.63701</td>\n",
       "      <td>0.52656</td>\n",
       "      <td>0.45768</td>\n",
       "      <td>0.63701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>copulagan-LDA</td>\n",
       "      <td>0.65119</td>\n",
       "      <td>0.59334</td>\n",
       "      <td>0.54555</td>\n",
       "      <td>0.65119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ctgan-LDA</td>\n",
       "      <td>0.63639</td>\n",
       "      <td>0.57703</td>\n",
       "      <td>0.65650</td>\n",
       "      <td>0.63639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nbsynthetic-LDA</td>\n",
       "      <td>0.69479</td>\n",
       "      <td>0.67580</td>\n",
       "      <td>0.68568</td>\n",
       "      <td>0.69479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>smote-GAMs</td>\n",
       "      <td>0.67008</td>\n",
       "      <td>0.55214</td>\n",
       "      <td>0.63488</td>\n",
       "      <td>0.67008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adasyn-GAMs</td>\n",
       "      <td>0.67254</td>\n",
       "      <td>0.55596</td>\n",
       "      <td>0.65869</td>\n",
       "      <td>0.67254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>copulagan-GAMs</td>\n",
       "      <td>0.73792</td>\n",
       "      <td>0.68200</td>\n",
       "      <td>0.76323</td>\n",
       "      <td>0.73792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ctgan-GAMs</td>\n",
       "      <td>0.72234</td>\n",
       "      <td>0.64406</td>\n",
       "      <td>0.71380</td>\n",
       "      <td>0.72234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nbsynthetic-GAMs</td>\n",
       "      <td>0.92208</td>\n",
       "      <td>0.91817</td>\n",
       "      <td>0.91976</td>\n",
       "      <td>0.92208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy  F1score  Precision   Recall\n",
       "0         smote-Decision Tree   0.70109  0.66712    0.66591  0.70109\n",
       "1        adasyn-Decision Tree   0.71121  0.66591    0.68470  0.71121\n",
       "2     copulagan-Decision Tree   0.81410  0.80762    0.80844  0.81410\n",
       "3         ctgan-Decision Tree   0.78939  0.77514    0.77645  0.78939\n",
       "4   nbsynthetic-Decision Tree   0.94601  0.94464    0.94484  0.94601\n",
       "5           smote-Naive Bayes   0.65906  0.57410    0.56929  0.65906\n",
       "6          adasyn-Naive Bayes   0.67112  0.59540    0.60596  0.67112\n",
       "7       copulagan-Naive Bayes   0.68928  0.64308    0.65313  0.68928\n",
       "8           ctgan-Naive Bayes   0.59232  0.58701    0.62358  0.59232\n",
       "9     nbsynthetic-Naive Bayes   0.90571  0.90391    0.90366  0.90571\n",
       "10                  smote-LDA   0.64111  0.53352    0.47249  0.64111\n",
       "11                 adasyn-LDA   0.63701  0.52656    0.45768  0.63701\n",
       "12              copulagan-LDA   0.65119  0.59334    0.54555  0.65119\n",
       "13                  ctgan-LDA   0.63639  0.57703    0.65650  0.63639\n",
       "14            nbsynthetic-LDA   0.69479  0.67580    0.68568  0.69479\n",
       "15                 smote-GAMs   0.67008  0.55214    0.63488  0.67008\n",
       "16                adasyn-GAMs   0.67254  0.55596    0.65869  0.67254\n",
       "17             copulagan-GAMs   0.73792  0.68200    0.76323  0.73792\n",
       "18                 ctgan-GAMs   0.72234  0.64406    0.71380  0.72234\n",
       "19           nbsynthetic-GAMs   0.92208  0.91817    0.91976  0.92208"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jm_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
