{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 외부데이터세트로 일반화 성능테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에 전혀 사용하지 않은 모집단을 제주 B초등학교에서 추가로 수집하여  \n",
    "서울 A초등학교로 학습한 모델이 이 외부데이터 세트인 B초등학교에서도 성능이 잘 나오는지 테스트하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score ## F1 Score 구하기\n",
    "from sklearn.metrics import precision_recall_fscore_support as sk\n",
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "path=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\jupyter\\\\Explainable Healthcare framework\\\\Explainable_healthcareframework'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제주 B초등학교 테스트 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_data=pd.read_csv(path+\"\\\\data\\\\5.제주초_df_final.csv\",encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID 라벨 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datapreprocessing.id_labelencoding import LabelEncoderWrapper \n",
    "\n",
    "df_encoder = LabelEncoderWrapper(jeju_data, 'ID')\n",
    "df_encoder.fit()\n",
    "df_encoder.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m day n term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일별 데이터를 다루기 때문에 처음에 접근한 방법으로는 몸무게의 변동이 거의 없을 수 밖에 없었다.   \n",
    "그래서 14일치 묶음으로 1일 윈도우로 이동하면서 몸무게 변동의 유의미한 차이를 만들기 위한 데이터세트를 구축하였다.  \n",
    "따라서 한 로우에 14일치 평균 그리고 다음 로우는 그 다음날부터 14일치 평균값이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datapreprocessing.M_day_N_term import RollingAveragesCalculator\n",
    "M_day=14\n",
    "n_term=1\n",
    "columns_to_average = [\"ID\",'height', 'weight', 'step count', 'burned calorie', 'eat calorie', 'sleep time']\n",
    "calculator = RollingAveragesCalculator(jeju_data)\n",
    "result_df = calculator.calculate_averages(columns_to_average, M_day, n_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14day_1term\n"
     ]
    }
   ],
   "source": [
    "name='{}day_{}term'.format(M_day,n_term)\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라벨링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라벨링은 임계값 몸무게 변동 차이 100g을 기준으로 이전 평균 몸무게 보다 증가하면 3, 유지면 2, 감소면 1로 라벨링하였다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID  height     weight   step count  burned calorie  eat calorie  \\\n",
      "0      0   143.0  33.392857  7001.748813       52.983568  3082.788763   \n",
      "1      0   143.0  33.428571  7179.177385       50.842598  2967.285192   \n",
      "2      0   143.0  33.464286  7048.891670       48.194152  2946.285192   \n",
      "3      0   143.0  33.492857  6741.605956       48.930571  3092.367841   \n",
      "4      0   143.0  33.521429  6598.748813       47.395000  3331.906238   \n",
      "...   ..     ...        ...          ...             ...          ...   \n",
      "3263  75   150.4  44.564286   686.285714       58.310000  2613.357143   \n",
      "3264  75   150.4  44.621429   704.857143       58.310000  2754.357143   \n",
      "3265  75   150.4  44.678571   722.976190       58.310000  2854.892857   \n",
      "3266  75   150.4  44.735714   740.642857       58.310000  2914.964286   \n",
      "3267  75   150.4  44.792857   757.857143       58.310000  2934.571429   \n",
      "\n",
      "      sleep time  Start_Day Label  \n",
      "0     437.964286          0     2  \n",
      "1     441.178571          1     2  \n",
      "2     447.178571          2     2  \n",
      "3     452.035714          3     2  \n",
      "4     455.607143          4     2  \n",
      "...          ...        ...   ...  \n",
      "3263  296.571429         38     2  \n",
      "3264  306.857143         39     2  \n",
      "3265  316.363636         40     2  \n",
      "3266  325.090909         41     2  \n",
      "3267  333.038961         42     2  \n",
      "\n",
      "[3268 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "from Datapreprocessing.labeling import WeightChangeLabeler\n",
    "\n",
    "Threshhold=0.1 # 몸무게가 얼마나 차이나야 변했다고 정할 것 인지 정하는 임계값\n",
    "labeler = WeightChangeLabeler(result_df, Threshhold)\n",
    "labeled_df = labeler.label_weight_changes()\n",
    "print(labeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df['Label']=labeled_df['Label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "2    2859\n",
       "3     266\n",
       "1     143\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  데이터 세트 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=labeled_df.iloc[:,[1,2,3,4,5,6,-1]]\n",
    "train=data.iloc[:,:-1]\n",
    "train_targets=data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 세트 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=train.to_numpy()\n",
    "y_test=train_targets-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_np = X_test\n",
    "\n",
    "test_targets_np=y_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 생성 method 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generation_list = ['smote','adasyn','copulagan','ctgan','nbsynthetic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제안한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "노트북 경로: c:\\jupyter\\Explainable Healthcare framework\\Explainable_healthcareframework\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.abspath('.')\n",
    "print(\"노트북 경로:\", notebook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 특성 shape: (3268, 6)\n",
      "샘플별 마스크 평균 shape: (3268, 6)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 6, got 12",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 28\u001b[0m\n\u001b[0;32m     23\u001b[0m test_features \u001b[38;5;241m=\u001b[39m create_augmented_features(tabnet_model, test_np)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# TabNet 임베딩 추출\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# XGBoost로 예측\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[43mproposed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_features\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# predict=predict+1\u001b[39;00m\n\u001b[0;32m     31\u001b[0m unique_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(predict)))\n",
      "File \u001b[1;32mc:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\xgboost\\sklearn.py:1565\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[1;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     X: ArrayLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1562\u001b[0m     iteration_range: Optional[IterationRange] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1563\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[0;32m   1564\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity):\n\u001b[1;32m-> 1565\u001b[0m         class_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1572\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output_margin:\n\u001b[0;32m   1573\u001b[0m             \u001b[38;5;66;03m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m class_probs\n",
      "File \u001b[1;32mc:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\xgboost\\sklearn.py:1186\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1186\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[0;32m   1195\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaemin\\anaconda3\\envs\\jm_main\\lib\\site-packages\\xgboost\\core.py:2524\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2520\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2521\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2522\u001b[0m         )\n\u001b[0;32m   2523\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features() \u001b[38;5;241m!=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m-> 2524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2525\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape mismatch, expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2526\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2527\u001b[0m         )\n\u001b[0;32m   2529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_np_array_like(data):\n\u001b[0;32m   2530\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ensure_np_dtype\n",
      "\u001b[1;31mValueError\u001b[0m: Feature shape mismatch, expected: 6, got 12"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from encoding_tabnet import train_tabnet_classifier, create_augmented_features\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import standardize_features, calculate_metrics, save_model, print_results\n",
    "\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    # TabNet 모델 로드\n",
    "    with open(path+f'\\\\3_1.Training_XAI_Proposed\\\\model_hist\\\\{generation}_proposed_tabnetEmbedd2.pickle', 'rb') as f:\n",
    "        tabnet_model = pickle.load(f)\n",
    "            \n",
    "    # XGBoost 모델 로드 \n",
    "    with open(path+f'\\\\3_1.Training_XAI_Proposed\\\\model_hist\\\\{generation}_proposed.pickle', 'rb') as f:\n",
    "        proposed_model = pickle.load(f)\n",
    "\n",
    "    # 테스트 데이터를 텐서로 변환\n",
    "    test_features = create_augmented_features(tabnet_model, test_np)\n",
    "    \n",
    "    # TabNet 임베딩 추출\n",
    "    \n",
    "    # XGBoost로 예측\n",
    "    predict = proposed_model.predict(test_features).astype(int)\n",
    "    \n",
    "    # predict=predict+1\n",
    "    unique_values = np.array(list(set(predict)))\n",
    "    print(f\"Unique values in prediction: {unique_values}\")\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    accuracy = accuracy_score(test_targets_np, predict)\n",
    "    f1 = f1_score(test_targets_np, predict, average='weighted')\n",
    "    precision, recall = sk(test_targets_np, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    results.append({\n",
    "        'Model': f\"{generation}-proposed_external\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5), \n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    })\n",
    "\n",
    "df_proposed_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusion matrix 계산\n",
    "cm = confusion_matrix(test_targets_np, predict)\n",
    "\n",
    "# Confusion matrix 시각화\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(test_targets_np, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proposed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "df_proposed_results.to_csv('proposed_model_metrics_externaltest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    with open(path+f'\\\\3_2.Training_XAI_VariousModel\\\\model_hist\\\\{generation}_proposed_tabnet_model.pickle', 'rb') as f:\n",
    "        Tabnet_model = pickle.load(f)\n",
    "        \n",
    "    predict=Tabnet_model.predict(test_np).astype(int)\n",
    "    # 각 메트릭 계산\n",
    "    accuracy = accuracy_score(test_targets_np, predict)\n",
    "    f1 = f1_score(test_targets_np, predict, average='weighted')\n",
    "    precision, recall = sk(test_targets_np, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-tabnet\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "\n",
    "df_tabnet_results = pd.DataFrame(results)\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tabnet_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusion matrix 계산\n",
    "cm = confusion_matrix(test_targets_np, predict)\n",
    "\n",
    "# Confusion matrix 시각화\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    with open(path+f'\\\\3_2.Training_XAI_VariousModel\\\\model_hist\\\\{generation}_DT_model.pickle', 'rb') as f:\n",
    "        DT_model = pickle.load(f)\n",
    "        \n",
    "    predict=DT_model.predict(test_np).astype(int)\n",
    "    # 각 메트릭 계산\n",
    "    accuracy = DT_model.score(test_np, test_targets_np)\n",
    "    f1 = f1_score(test_targets_np, predict, average='weighted')\n",
    "    precision, recall = sk(test_targets_np, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-Decision Tree\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_DT_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DT_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    with open(path+f'\\\\3_2.Training_XAI_VariousModel\\\\model_hist\\\\{generation}_NB_model.pickle', 'rb') as f:\n",
    "        NB_model = pickle.load(f)\n",
    "    predict=NB_model.predict(test_np).astype(int)\n",
    "    # 각 메트릭 계산\n",
    "    accuracy = NB_model.score(test_np, test_targets_np)\n",
    "    f1 = f1_score(test_targets_np, predict, average='weighted')\n",
    "    precision, recall = sk(test_targets_np, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-Naive Bayes\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_Naive_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Naive_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDA_model= LinearDiscriminantAnalysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    with open(path+f'\\\\3_2.Training_XAI_VariousModel\\\\model_hist\\\\{generation}_LDA_model.pickle', 'rb') as f:\n",
    "        LDA_model = pickle.load(f)\n",
    "    \n",
    "    with open(path+f'\\\\3_2.Training_XAI_VariousModel\\\\scaler_hist\\\\{generation}_LDA_scaler.pickle', 'rb') as f:\n",
    "        LDA_scaler = pickle.load(f)\n",
    "    LDA_scaler_test_np=LDA_scaler.transform(test_np)    \n",
    "        \n",
    "        \n",
    "    predict=LDA_model.predict(LDA_scaler_test_np).astype(int)\n",
    "    # 각 메트릭 계산\n",
    "    accuracy = LDA_model.score(LDA_scaler_test_np, test_targets_np)\n",
    "    f1 = f1_score(test_targets_np, predict, average='weighted')\n",
    "    precision, recall = sk(test_targets_np, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-LDA\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_LDA_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LDA_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from pygam import LogisticGAM, s, f\n",
    "\n",
    "def train_multiclass_gam(X_test, y_test):\n",
    "    # 레이블 인코딩\n",
    "    le = LabelEncoder()\n",
    "    y_test_encoded = le.fit_transform(y_test)\n",
    "    n_classes = len(np.unique(y_test_encoded))\n",
    "\n",
    "    return y_test_encoded, le\n",
    "\n",
    "def predict_multiclass(gam_models, X):\n",
    "    # 각 클래스에 대한 예측 확률 계산\n",
    "    probs = np.column_stack([\n",
    "        gam.predict_proba(X) for gam in gam_models\n",
    "    ])\n",
    "    \n",
    "    # 가장 높은 확률을 가진 클래스 선택\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "def print_model_summary(gam_models, class_names):\n",
    "    print(\"\\n모델 요약:\")\n",
    "    for i, gam in enumerate(gam_models):\n",
    "        print(f\"\\n클래스 {class_names[i]}:\")\n",
    "        print(\"특성별 계수 정보:\")\n",
    "        for j, coef in enumerate(gam.coef_):\n",
    "            print(f\"특성 {j + 1} - 계수: {coef:.4f}\")\n",
    "        print(\"\\n모델 summary:\")\n",
    "        gam.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 리스트 생성하여 결과 저장\n",
    "results = []\n",
    "\n",
    "for generation in Generation_list:\n",
    "    with open(path+f'\\\\3_2.Training_XAI_VariousModel\\\\model_hist\\\\{generation}_gams_model.pickle', 'rb') as f:\n",
    "        GAMs_model = pickle.load(f)\n",
    "    y_test_encoded, le = train_multiclass_gam(train, train_targets)\n",
    "    predict = predict_multiclass(GAMs_model, train)\n",
    "    accuracy = accuracy_score(y_test_encoded, predict)\n",
    " \n",
    "    # 각 메트릭 계산\n",
    "    f1 = f1_score(y_test_encoded, predict, average='weighted')\n",
    "    precision, recall = sk(y_test_encoded, predict, beta=1, average='weighted')[:2]\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result_dict = {\n",
    "        'Model': f\"{generation}-GAMs\",\n",
    "        'Accuracy': round(accuracy, 5),\n",
    "        'F1score': round(f1, 5),\n",
    "        'Precision': round(precision, 5),\n",
    "        'Recall': round(recall, 5)\n",
    "    }\n",
    "    \n",
    "    # 리스트에 딕셔너리 추가\n",
    "    results.append(result_dict)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df_GAMs_results = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택사항)\n",
    "# df_results.to_csv('model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GAMs_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 총합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LDA_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml=pd.concat([df_proposed_results,df_tabnet_results, df_DT_results,df_Naive_results,df_LDA_results,df_GAMs_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml=df_ml.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.to_csv(path+'\\\\data\\\\머신 러닝 일반화 성능 정리_제주초.csv',encoding='cp949',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jm_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
